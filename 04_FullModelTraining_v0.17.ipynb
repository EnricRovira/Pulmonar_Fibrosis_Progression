{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EncoderDecoder Sequence Fibrosis Progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# 01. Libraries\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# To allocate memory dynamically\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "# tf.config.experimental.enable_mlir_graph_optimization()\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers, constraints, initializers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from Utils.utils import *\n",
    "from Utils.attention_layers import BahdanauAttention, ScaledDotProductAttention, GeneralAttention, VisualAttentionBlock\n",
    "from Utils.preprocess_scans import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy as sp\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# 02. Global Variables\n",
    "\n",
    "path = '../01_Data/'\n",
    "path_models = '../05_Saved_Models/'\n",
    "\n",
    "path_train_masks = path + '/train_masks_fast_masks/'\n",
    "path_test_masks = path + '/test_masks_fast_masks/'\n",
    "\n",
    "path_scans_train = path + 'train/'\n",
    "path_scans_test = path + 'test/'\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 176/176 [00:00<00:00, 44108.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 176/176 [00:00<00:00, 8831.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 4890.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 -> There are 176 train unique patients\n",
      "1.2 -> There are 5 test unique patients\n",
      "No. of Train Masks : 176\n",
      "No. of Test Masks : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# 03. Load Data & Preprocess Data\n",
    "\n",
    "df_train = pd.read_csv( path + 'train.csv')\n",
    "df_test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "print(f'1.1 -> There are {df_train.Patient.unique().shape[0]} train unique patients')\n",
    "print(f'1.2 -> There are {df_test.Patient.unique().shape[0]} test unique patients')\n",
    "\n",
    "train_mask_paths = glob.glob(path_train_masks + '*')\n",
    "test_mask_paths = glob.glob(path_test_masks + '*')\n",
    "\n",
    "print(f'No. of Train Masks : {len(train_mask_paths)}')\n",
    "print(f'No. of Test Masks : {len(test_mask_paths)}')\n",
    "      \n",
    "unique_train_patients = df_train.Patient.unique()\n",
    "unique_test_patients = df_test.Patient.unique()\n",
    "\n",
    "train_patients = os.listdir(path_train_masks)\n",
    "test_patients = os.listdir(path_test_masks)\n",
    "\n",
    "dict_train_patients_masks_paths = {patient: path_train_masks + patient + '/' for patient in train_patients}\n",
    "dict_test_patients_masks_paths = {patient: path_test_masks + patient + '/' for patient in test_patients}\n",
    "\n",
    "dict_train_patients_scans_paths = {patient: path_scans_train + patient + '/' for patient in unique_train_patients}\n",
    "dict_test_patients_scans_paths = {patient: path_scans_test + patient + '/' for patient in unique_test_patients}\n",
    "\n",
    "for patient in tqdm(dict_train_patients_masks_paths):\n",
    "    list_files = os.listdir(dict_train_patients_masks_paths[patient])\n",
    "    list_files = [dict_train_patients_masks_paths[patient] + file for file in list_files]\n",
    "    dict_train_patients_masks_paths[patient] = list_files\n",
    "    \n",
    "for patient in tqdm(dict_test_patients_masks_paths):\n",
    "    list_files = os.listdir(dict_test_patients_masks_paths[patient])\n",
    "    list_files = [dict_test_patients_masks_paths[patient] + file for file in list_files]\n",
    "    dict_test_patients_masks_paths[patient] = list_files\n",
    "    \n",
    "\n",
    "for patient in tqdm(dict_train_patients_scans_paths):\n",
    "    list_files = os.listdir(dict_train_patients_scans_paths[patient])\n",
    "    list_files = [dict_train_patients_scans_paths[patient] + file for file in list_files]\n",
    "    dict_train_patients_scans_paths[patient] = list_files\n",
    "    \n",
    "for patient in tqdm(dict_test_patients_scans_paths):\n",
    "    list_files = os.listdir(dict_test_patients_scans_paths[patient])\n",
    "    list_files = [dict_test_patients_scans_paths[patient] + file for file in list_files]\n",
    "    dict_test_patients_scans_paths[patient] = list_files\n",
    "    \n",
    "# Preprocessing:\n",
    "\n",
    "df_train = df_train.groupby(['Patient', 'Weeks']).agg({\n",
    "    'FVC': np.mean,\n",
    "    'Percent': np.mean,\n",
    "    'Age': np.max,\n",
    "    'Sex': np.max,\n",
    "    'SmokingStatus': np.max \n",
    "}).reset_index()\n",
    "\n",
    "df_train['FVC_Percent'] = (df_train['FVC'] / df_train['Percent']) * 100\n",
    "df_test['FVC_Percent'] = (df_test['FVC'] / df_test['Percent']) * 100\n",
    "\n",
    "# Standarize data\n",
    "\n",
    "mean_fvc, std_fvc = df_train.FVC.mean(), df_train.FVC.std()\n",
    "mean_perc, std_perc = df_train.Percent.mean(), df_train.Percent.std()\n",
    "mean_age, std_age = df_train.Age.mean(), df_train.Age.std()\n",
    "\n",
    "df_train['Age'] = df_train['Age'].apply(lambda x: scale(x, mean_age, std_age))\n",
    "df_test['Age'] = df_test['Age'].apply(lambda x: scale(x, mean_age, std_age))\n",
    "\n",
    "df_train['FVC'] = df_train['FVC'].apply(lambda x: scale(x, mean_fvc, std_fvc))\n",
    "df_test['FVC'] = df_test['FVC'].apply(lambda x: scale(x, mean_fvc, std_fvc))\n",
    "\n",
    "df_train['FVC_Percent'] = df_train['FVC_Percent'].apply(lambda x: scale(x, mean_fvc, std_fvc))\n",
    "df_test['FVC_Percent'] = df_test['FVC_Percent'].apply(lambda x: scale(x, mean_fvc, std_fvc))\n",
    "\n",
    "df_train['Percent'] = df_train['Percent'].apply(lambda x: scale(x, mean_perc, std_perc))\n",
    "df_test['Percent'] = df_test['Percent'].apply(lambda x: scale(x, mean_perc, std_perc))\n",
    "\n",
    "# Mapping categories dictionaries \n",
    "\n",
    "dict_sex = {'Male': 0, 'Female': 1}\n",
    "dict_sex_inv = {0: 'Male', 1: 'Female'}\n",
    "\n",
    "dict_smoke = {'Ex-smoker': 0, 'Never smoked': 1, 'Currently smokes': 2}\n",
    "dict_smoke_inv = {0: 'Ex-smoker', 1:'Never smoked', 2:'Currently smokes'}\n",
    "\n",
    "dict_kind_patient = {'decreased': 0, 'regular': 1, 'increased': 2}\n",
    "dict_kind_patient_inv = {0: 'decreased', 1: 'regular', 2: 'increased'}\n",
    "\n",
    "df_train.Sex = df_train.Sex.apply(lambda x: dict_sex[x])\n",
    "df_train.SmokingStatus = df_train.SmokingStatus.apply(lambda x: dict_smoke[x])\n",
    "\n",
    "df_test.Sex = df_test.Sex.apply(lambda x: dict_sex[x])\n",
    "df_test.SmokingStatus = df_test.SmokingStatus.apply(lambda x: dict_smoke[x])\n",
    "\n",
    "# Build WeeksSinceLastVisit feature\n",
    "\n",
    "df_train['ElapsedWeeks'] = df_train['Weeks']\n",
    "df_test['ElapsedWeeks'] = df_test['Weeks']\n",
    "\n",
    "train_weeks_elapsed = df_train.set_index(['Patient', 'Weeks'])['ElapsedWeeks'].diff().reset_index()\n",
    "test_weeks_elapsed = df_test.set_index(['Patient', 'Weeks'])['ElapsedWeeks'].diff().reset_index()\n",
    "\n",
    "df_train = df_train.drop('ElapsedWeeks', axis=1)\n",
    "df_test = df_test.drop('ElapsedWeeks', axis=1)\n",
    "\n",
    "train_weeks_elapsed['ElapsedWeeks'] = train_weeks_elapsed['ElapsedWeeks'].fillna(0).astype(int)\n",
    "test_weeks_elapsed['ElapsedWeeks'] = test_weeks_elapsed['ElapsedWeeks'].fillna(0).astype(int)\n",
    "\n",
    "df_train = df_train.merge(train_weeks_elapsed, how='inner', on=['Patient', 'Weeks'])\n",
    "df_test = df_test.merge(test_weeks_elapsed, how='inner', on=['Patient', 'Weeks'])\n",
    "\n",
    "df_train['patient_row'] = df_train.sort_values(['Patient', 'Weeks'], ascending=[True, True]) \\\n",
    "             .groupby(['Patient']) \\\n",
    "             .cumcount() + 1\n",
    "\n",
    "df_test['patient_row'] = df_test.sort_values(['Patient', 'Weeks'], ascending=[True, True]) \\\n",
    "             .groupby(['Patient']) \\\n",
    "             .cumcount() + 1\n",
    "\n",
    "df_train['WeeksSinceLastVisit'] = df_train.apply(lambda x: x['Weeks'] if x['patient_row']==1 else x['ElapsedWeeks'], axis=1)\n",
    "df_test['WeeksSinceLastVisit'] = df_test.apply(lambda x: x['Weeks'] if x['patient_row']==1 else x['ElapsedWeeks'], axis=1)\n",
    "\n",
    "# Norm Weeks\n",
    "\n",
    "mean_weeks, std_weeks = df_train.Weeks.min(), df_train.Weeks.max()\n",
    "\n",
    "df_train['WeeksSinceLastVisit'] = df_train['WeeksSinceLastVisit'].apply(lambda x: scale(x, mean_weeks, std_weeks))\n",
    "df_test['WeeksSinceLastVisit'] = df_test['WeeksSinceLastVisit'].apply(lambda x: scale(x, mean_weeks, std_weeks))\n",
    "\n",
    "\n",
    "df_train['Weeks'] = df_train['Weeks'].apply(lambda x: scale(x, mean_weeks, std_weeks))\n",
    "df_test['Weeks'] = df_test['Weeks'].apply(lambda x: scale(x, mean_weeks, std_weeks))\n",
    "\n",
    "# Ini dictionaries\n",
    "\n",
    "columns = ['FVC', 'Age', 'Sex', 'SmokingStatus', 'WeeksSinceLastVisit', 'Percent']\n",
    "dict_patients_train_ini_features, dict_patients_test_ini_features = {}, {}\n",
    "dict_patients_train_kind_patient, dict_patients_test_kind_patient = {}, {}\n",
    "df_train_patients, df_test_patients = df_train.set_index('Patient'), df_test.set_index('Patient')\n",
    "\n",
    "for patient in unique_train_patients:\n",
    "    dict_patients_train_ini_features[patient] = df_train_patients[columns][df_train_patients.index==patient].\\\n",
    "                                                                    to_dict('records')[0]\n",
    "    std = np.std(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values)\n",
    "    mean_first_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[:1])\n",
    "    mean_last_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[-1:])\n",
    "    if std<=100:\n",
    "        dict_patients_train_kind_patient[patient] = 'regular'\n",
    "    elif std>100 and mean_last_1 > mean_first_1 :\n",
    "        dict_patients_train_kind_patient[patient] = 'increased'\n",
    "    elif std>100 and mean_last_1 <= mean_first_1 :\n",
    "        dict_patients_train_kind_patient[patient] = 'decreased'\n",
    "    dict_patients_train_ini_features[patient]['kind'] = dict_kind_patient[dict_patients_train_kind_patient[patient]]\n",
    "        \n",
    "    \n",
    "for patient in unique_test_patients:\n",
    "    dict_patients_test_ini_features[patient] = df_test_patients[columns][df_test_patients.index==patient].\\\n",
    "                                                                    to_dict('records')[0]\n",
    "    std = np.std(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values)\n",
    "    mean_first_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[:1])\n",
    "    mean_last_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[-1:])\n",
    "    if std<=100:\n",
    "        dict_patients_test_kind_patient[patient] = 'regular'\n",
    "    elif std>100 and mean_last_1 > mean_first_1 :\n",
    "        dict_patients_test_kind_patient[patient] = 'increased'\n",
    "    elif std>100 and mean_last_1 <= mean_first_1 :\n",
    "        dict_patients_test_kind_patient[patient] = 'decreased'\n",
    "    dict_patients_test_ini_features[patient]['kind'] = dict_kind_patient[dict_patients_test_kind_patient[patient]]\n",
    "\n",
    "# Decoder inputs\n",
    "\n",
    "dict_train_sequence_fvc, dict_train_sequence_weekssincelastvisit = {}, {}\n",
    "dict_train_sequence_cumweeks = {}\n",
    "for patient in unique_train_patients:\n",
    "    dict_train_sequence_fvc[patient] = list(df_train_patients['FVC'].loc[patient].values[1:])\n",
    "    dict_train_sequence_weekssincelastvisit[patient] = list(df_train_patients['WeeksSinceLastVisit'].loc[patient].values[1:])\n",
    "    dict_train_sequence_cumweeks[patient] = list(df_train_patients['Weeks'].loc[patient].values[1:])\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generator\n",
    "\n",
    "Similar as `03_Autoencoder` Training Generator but instead of imgs as output we will have the ini features that we will use as our encoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "\n",
    "## 04. Data Generator\n",
    "\n",
    "class ForecastTabularImgDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, raw_scans, training, patients, df_tabular, dict_ini_features, dict_patients_masks_paths,\n",
    "                 batch_size=1, num_frames_batch=32, dict_raw_scans_paths=None, \n",
    "                 alpha=1.0, random_window=False, center_crop=True,\n",
    "                 img_size_load=(500, 500, 3), \n",
    "                 img_size_crop=(440, 440, 3)):\n",
    "        \n",
    "        super(ForecastTabularImgDataGenerator, self).__init__()\n",
    "        self.raw_scans = raw_scans\n",
    "        self.training = training\n",
    "        self.dict_ini_features = dict_ini_features\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frames_batch = num_frames_batch\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.random_window = random_window\n",
    "        self.center_crop = center_crop\n",
    "        self.img_size_load = img_size_load\n",
    "        self.img_size_crop = img_size_crop\n",
    "        \n",
    "        self.dict_patients_masks_paths = dict_patients_masks_paths\n",
    "        self.dict_raw_scans_paths = dict_raw_scans_paths\n",
    "        \n",
    "        self.df_tabular = df_tabular\n",
    "        self.ids = list(df_tabular.index)\n",
    "        self.num_steps = int(np.ceil(len(self.ids) / self.batch_size))\n",
    "        self.last_patient = ''\n",
    "        self.on_epoch_end()\n",
    "      \n",
    "    # Number of batches in the sequence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "    \n",
    "    \n",
    "    # Gets the batch at position index, return patient images and dict ini features\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        patient_ids = list(self.df_tabular['Patient'].loc[indexes].unique()) # [self.ids[k] for k in indexes]\n",
    "        if not self.raw_scans:\n",
    "            list_scan_imgs = [decodePatientImages(patient, \n",
    "                                                  self.dict_patients_masks_paths,\n",
    "                                                  image_size=(self.img_size_load[0], self.img_size_load[1]), \n",
    "                                                  numpy=True) \n",
    "                              for patient in patient_ids]\n",
    "        else:\n",
    "            list_scan_imgs = self.preprocessRawScans(patient_ids)\n",
    "           \n",
    "        patient_imgs = self.groupImages(list_scan_imgs)\n",
    "        patient_imgs = self.loadImagesAugmented(patient_imgs)\n",
    "\n",
    "        for patient_ in patient_ids:\n",
    "            self.dict_ini_features[patient_]['Patient'] = patient_\n",
    "        features = list(col for col in self.df_tabular.columns if col not in ['Patient', 'fvc_real'])\n",
    "\n",
    "        patient_tabular_features = self.df_tabular[features].loc[indexes].values\n",
    "        patient_y = np.expand_dims(self.df_tabular['fvc_real'].loc[indexes].values, -1)\n",
    "        \n",
    "        return (patient_imgs, patient_tabular_features, patient_y)\n",
    "    \n",
    "    \n",
    "    # Preprocess Raw Scans in dicom format\n",
    "    \n",
    "    def preprocessRawScans(self, patient_ids):\n",
    "        patients_files = [self.dict_raw_scans_paths[patient] for patient in patient_ids]\n",
    "        patients_slices = [loadSlices(p) for p in patients_files]\n",
    "        patients_images = [getPixelsHu(p_slices) for p_slices in patients_slices]\n",
    "        patients_resampled_imgs = [resampleImages(p_images, p_slice, [1, 1, 1])[0] \\\n",
    "                                            for p_images, p_slice in zip(patients_images, patients_slices)]\n",
    "        patients_crop_imgs = [np.asarray([imCropCenter(img, 320, 320) for img in p_resampled_imgs]) \\\n",
    "                              for p_resampled_imgs in patients_resampled_imgs]\n",
    "        patients_segmented_lungs_fill = [np.asarray([seperateLungs(img, n_iters=2, only_internal=False, only_watershed=True)\n",
    "                                                    for img in p_crop_imgs]) for p_crop_imgs in patients_crop_imgs]\n",
    "        patients_masked_imgs = [np.where(p_lungs_fill==255, p_imgs, -2_048) \\\n",
    "                                for p_lungs_fill, p_imgs in zip(patients_segmented_lungs_fill, patients_crop_imgs)]\n",
    "        \n",
    "        patients_imgs = [windowImageNorm(p_imgs, min_bound=-1_000, max_bound=400) for p_imgs in patients_masked_imgs]\n",
    "        patients_imgs = [tf.convert_to_tensor(img, dtype=tf.float32) for img in patients_imgs]\n",
    "        patients_img_resized = [tf.convert_to_tensor([tf.image.resize(tf.expand_dims(img, axis=2), \n",
    "                                                                      (self.img_size_load[0], self.img_size_load[1])) \n",
    "                                                      for img in p_imgs], \n",
    "                                           dtype=tf.float32) for p_imgs in patients_imgs]\n",
    "        return patients_img_resized\n",
    "        \n",
    "    \n",
    "    # From n patient frames we will only keep self.alpha*n frames, cutting on top and bottom\n",
    "    \n",
    "    def filterSlices(self, array_imgs):\n",
    "        num_patient_slices = array_imgs.shape[0]\n",
    "        beta = int(self.alpha * num_patient_slices)\n",
    "        if beta % 2 != 0:\n",
    "            beta += 1\n",
    "        if num_patient_slices > self.num_frames_batch:\n",
    "            if beta > self.num_frames_batch and self.alpha < 1:\n",
    "                remove = int((num_patient_slices - beta)/2)\n",
    "                array_imgs = array_imgs[remove:, :, :, :]\n",
    "                array_imgs = array_imgs[:-remove:, :, :]\n",
    "\n",
    "        return array_imgs\n",
    "    \n",
    "    # Skip frames unniformally according to self.num_frames_batch value\n",
    "    \n",
    "    def frameSkipImages(self, patient_imgs):\n",
    "        num_patient_slices = patient_imgs.shape[0]\n",
    "        frame_skip = num_patient_slices // self.num_frames_batch\n",
    "        skipped_patient_imgs = np.zeros((self.num_frames_batch, self.img_size_load[0], self.img_size_load[1], 1))\n",
    "        for i in range(self.num_frames_batch):\n",
    "            skipped_patient_imgs[i] = patient_imgs[i*frame_skip]    \n",
    "        return skipped_patient_imgs\n",
    "    \n",
    "    # Select a random window of patient frames, in case its images has more frames than self.num_frame_batch \n",
    "    \n",
    "    def randomWindow(self, patient_imgs):\n",
    "        windowed_imgs = np.zeros((self.num_frames_batch, patient_imgs.shape[1], patient_imgs.shape[2], 1))\n",
    "        num_frames = patient_imgs.shape[0]\n",
    "        if num_frames < self.num_frames_batch:\n",
    "            windowed_imgs[:num_frames] = patient_imgs\n",
    "        else:\n",
    "            random_frames = np.arange(num_frames)\n",
    "            index = np.random.randint(0, num_frames - self.num_frames_batch)\n",
    "            windowed_imgs[0:] = patient_imgs[index:index+self.num_frames_batch]\n",
    "        return windowed_imgs\n",
    "            \n",
    "    \n",
    "    # Convert raw frames to a fix size array -> (batch_size, num_frames_batch, img_size_crop[0], img_size_crop[1], 1)\n",
    "    \n",
    "    def groupImages(self, list_scan_imgs):\n",
    "        grouped_imgs = []\n",
    "        for patient_imgs in list_scan_imgs:\n",
    "            if patient_imgs.shape[1] > self.num_frames_batch:\n",
    "                patient_imgs = self.filterSlices(patient_imgs)\n",
    "            if self.random_window:\n",
    "                patient_imgs = self.randomWindow(patient_imgs)\n",
    "            else:\n",
    "                patient_imgs = self.frameSkipImages(patient_imgs)\n",
    "            grouped_imgs.append(patient_imgs)\n",
    "        return np.asarray(grouped_imgs)\n",
    "        \n",
    "    # Performs augmentation operations conserving the 3D property on the z axis\n",
    "    \n",
    "    def loadImagesAugmented(self, patient_imgs):\n",
    "\n",
    "        if self.center_crop: #self.img_size_load != self.img_size_crop:\n",
    "            # patient_imgs = self.center3Dcropping(patient_imgs)\n",
    "            if patient_imgs.shape[2] > self.img_size_crop[0] and patient_imgs.shape[3] > self.img_size_crop[1]:\n",
    "                patient_imgs = self.random3DCropping(patient_imgs)\n",
    "        if self.training and np.random.random() > 0.5:\n",
    "            patient_imgs = np.fliplr(patient_imgs)\n",
    "        if self.training and np.random.random() > 0.5:\n",
    "            patient_imgs = np.flipud(patient_imgs)\n",
    "        if self.training and np.random.random() > 0.5:\n",
    "            patient_imgs = patient_imgs[:, :, ::-1]\n",
    "        if self.training and np.random.random() > 0.5:\n",
    "            patient_imgs = patient_imgs[:, ::-1, :]\n",
    "        if self.training:\n",
    "            patient_rotated_imgs= []\n",
    "            angle = np.random.randint(-15, 15)\n",
    "            for batch in range(patient_imgs.shape[0]):\n",
    "                batch_imgs_rotated = np.asarray([ndimage.rotate(patient_imgs[batch, i], angle, order=1,\n",
    "                                                                reshape=False) for i in range(patient_imgs.shape[1])])\n",
    "                patient_rotated_imgs.append(batch_imgs_rotated)\n",
    "            patient_imgs = np.asarray(patient_rotated_imgs) \n",
    "        return patient_imgs\n",
    "    \n",
    "    # gull Center 3d Cropping \n",
    "    \n",
    "    def fullcenter3DCropping(self, patient_imgs):\n",
    "        cropped_imgs = []\n",
    "        for batch in range(patient_imgs.shape[0]):\n",
    "            imgs = np.asarray([cropLung(patient_imgs[batch, img].squeeze()) for img in range(patient_imgs.shape[1])])\n",
    "            cropped_imgs.append(imgs)\n",
    "\n",
    "        return np.expand_dims(np.asarray(cropped_imgs), axis=-1)\n",
    "    \n",
    "    #Random Cropping 3D - change x, y axis but not z\n",
    "    \n",
    "    def random3DCropping(self, patient_imgs):\n",
    "        w, h = self.img_size_crop[0], self.img_size_crop[1]\n",
    "        x = np.random.randint(0, patient_imgs.shape[2] - w)\n",
    "        y = np.random.randint(0, patient_imgs.shape[2] - h)\n",
    "        patient_crop_imgs = patient_imgs[:, :, y:y+h, x:x+w]\n",
    "        return patient_crop_imgs\n",
    "    \n",
    "    # Center 3D Cropping\n",
    "    \n",
    "    def center3Dcropping(self, patient_imgs):\n",
    "        w, h = patient_imgs.shape[2] - 20, patient_imgs.shape[3] - 20\n",
    "        img_height, img_width = patient_imgs.shape[2], patient_imgs.shape[3]\n",
    "        left, right = (img_width - w) / 2, (img_width + w) / 2\n",
    "        top, bottom = (img_height - h) / 2, (img_height + h) / 2\n",
    "        left, top = round(max(0, left)), round(max(0, top))\n",
    "        right, bottom = round(min(img_width - 0, right)), round(min(img_height - 0, bottom))\n",
    "        patient_crop_imgs = patient_imgs[:, :, top:bottom, left:right]\n",
    "        return patient_crop_imgs\n",
    "    \n",
    "    # We shuffle the data at the end of each epoch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.ids))\n",
    "        np.random.shuffle(self.indexes)\n",
    "     \n",
    "    # Get only one patient, for debugging or prediction\n",
    "        \n",
    "    def getOnePatient(self, patient_id):\n",
    "        if not self.raw_scans:\n",
    "            list_scan_imgs = [decodePatientImages(patient_id, \n",
    "                                                  self.dict_patients_masks_paths,\n",
    "                                                  image_size=(self.img_size_load[0], self.img_size_load[1]), \n",
    "                                                  numpy=True)]\n",
    "        else:\n",
    "            list_scan_imgs = self.preprocessRawScans([patient_id])\n",
    "            \n",
    "        patient_imgs = self.groupImages(list_scan_imgs)\n",
    "        patient_imgs = self.loadImagesAugmented(patient_imgs)\n",
    "        self.dict_ini_features[patient_id]['Patient'] = patient_id\n",
    "        return (patient_imgs, [self.dict_ini_features[patient_id]])\n",
    "\n",
    "    \n",
    "def buildDataSet(list_patients, dict_ini_features, dict_seq_cumweeks, \n",
    "                 training=True, predictions=None):\n",
    "    \n",
    "    dict_to_tree = {\n",
    "        'Patient' : [],\n",
    "        'Weeks_Elapsed_since_firstVisit': [],\n",
    "        'Base_Percent' : [],\n",
    "        'Age' : [],\n",
    "        'Sex' : [],\n",
    "        'Base_Week' : [],\n",
    "        'Base_FVC' : [],\n",
    "        'SmokingStatus' : []\n",
    "    }\n",
    "\n",
    "    if training:\n",
    "        dict_to_tree['fvc_real'] = []\n",
    "    \n",
    "\n",
    "    for patient in tqdm(list_patients, position=0):\n",
    "        \n",
    "        dict_to_tree['Weeks_Elapsed_since_firstVisit'].extend([dict_seq_cumweeks[patient][i] \\\n",
    "                                            for i in range(len(dict_seq_cumweeks[patient]))])\n",
    "        \n",
    "        for i in range(len(dict_seq_cumweeks[patient])):\n",
    "            dict_to_tree['Patient'].extend([patient])\n",
    "\n",
    "            dict_to_tree['Base_Percent'].extend([dict_ini_features[patient]['Percent']])\n",
    "\n",
    "            dict_to_tree['Age'].extend([dict_ini_features[patient]['Age']])\n",
    "\n",
    "            dict_to_tree['Sex'].extend([dict_ini_features[patient]['Sex']])\n",
    "\n",
    "            dict_to_tree['Base_Week'].extend([dict_ini_features[patient]['WeeksSinceLastVisit']])\n",
    "\n",
    "            dict_to_tree['Base_FVC'].extend([dict_ini_features[patient]['FVC']])\n",
    "\n",
    "            dict_to_tree['SmokingStatus'].extend([dict_ini_features[patient]['SmokingStatus']])\n",
    "\n",
    "\n",
    "        if training:\n",
    "            dict_to_tree['fvc_real'].extend(dict_train_sequence_fvc[patient])\n",
    "\n",
    "    df_tree = pd.DataFrame.from_dict(dict_to_tree, orient='columns')\n",
    "    \n",
    "    return df_tree\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackBoneModel(models.Model):\n",
    "    def __init__(self, img_dim=128, tabular_dim=64, features_dim=[32, 16], \n",
    "                 dropouts=[0.3, 0.2],\n",
    "                 l2_reg=1e-4, batch_norm=False, max_norm=1,\n",
    "                 path_img_model='./', **kwargs):\n",
    "        \n",
    "        super(BackBoneModel, self).__init__(**kwargs, name='BackBoneModel')\n",
    "        \n",
    "        self.features_dim = features_dim\n",
    "        self.dropouts = dropouts\n",
    "        self.l2_reg = l2_reg\n",
    "        self.max_norm = max_norm\n",
    "        self.dense_img = layers.Dense(img_dim,\n",
    "                                      activation=None,\n",
    "                                      kernel_constraint=constraints.MaxNorm(self.max_norm),\n",
    "                                      bias_constraint=constraints.MaxNorm(self.max_norm),\n",
    "                                      kernel_regularizer=regularizers.l2(self.l2_reg), \n",
    "                                      bias_regularizer=regularizers.l2(self.l2_reg))\n",
    "        \n",
    "        self.emb_sex = layers.Embedding(input_dim=2, output_dim=20, \n",
    "                                       embeddings_regularizer=regularizers.l2(1e-4))\n",
    "                                    \n",
    "        self.emb_smoker = layers.Embedding(input_dim=3, output_dim=20,\n",
    "                                           embeddings_regularizer=regularizers.l2(1e-4))\n",
    "        \n",
    "        self.dense_tab = layers.Dense(features_dim[0],\n",
    "                                                  activation=None,\n",
    "                                                  kernel_regularizer=regularizers.l2(self.l2_reg), \n",
    "                                                  bias_regularizer=regularizers.l2(self.l2_reg))\n",
    "        \n",
    "        self.batch_norm = batch_norm\n",
    "        self.img_bn = layers.BatchNormalization()\n",
    "        self.tab_bn = layers.BatchNormalization()\n",
    "        self.dropouts = dropouts\n",
    "        \n",
    "        if self.features_dim:\n",
    "            self.fcc_denses, self.fcc_batch_norms = self.stackDense()\n",
    "        if len(self.dropouts)>1:\n",
    "            self.fcc_dropouts = self.stackDropout()\n",
    "    \n",
    "        self.img_model = models.load_model(path_img_model, compile=False)\n",
    "        \n",
    "        self.output_1 = layers.Dense(3, activation='linear', kernel_regularizer=regularizers.l2(self.l2_reg))\n",
    "        self.output_2 = layers.Dense(3, activation='relu', kernel_regularizer=regularizers.l2(self.l2_reg))\n",
    "        \n",
    "        self.output_total = layers.Lambda(lambda x: x[0] + tf.cumsum(x[1]), name='quantile_preds')\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        img_inputs, tabular_inputs = inputs\n",
    "        x = img_inputs\n",
    "        \n",
    "        x = self.img_model(x, training)\n",
    "        x = self.dense_img(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.img_bn(x, training)\n",
    "        img_features = tf.nn.relu(x)\n",
    "        \n",
    "        #####\n",
    "        patient_sex = self.emb_sex(tabular_inputs[:, 3])\n",
    "        patient_smoke = self.emb_smoker(tabular_inputs[:, 6])\n",
    "        \n",
    "        tab_features = tf.concat([patient_sex,\n",
    "                                   patient_smoke,\n",
    "                                   tf.expand_dims(tabular_inputs[:, 0], 1),\n",
    "                                   tf.expand_dims(tabular_inputs[:, 1], 1),\n",
    "                                   tf.expand_dims(tabular_inputs[:, 2], 1),\n",
    "                                   tf.expand_dims(tabular_inputs[:, 4], 1),\n",
    "                                   tf.expand_dims(tabular_inputs[:, 5], 1)], \n",
    "                axis=-1) \n",
    "        \n",
    "        tab_features = self.dense_tab(tab_features)\n",
    "        if self.batch_norm:\n",
    "            tab_features = self.tab_bn(tab_features, training)\n",
    "        tab_features = tf.nn.relu(tab_features)\n",
    "\n",
    "        ####\n",
    "        x = tf.concat([img_features, tab_features], axis=-1)\n",
    "        ####\n",
    "        \n",
    "        if self.features_dim:\n",
    "            for i, (fcc_dense, fcc_drop) in enumerate(zip(self.fcc_denses, self.fcc_dropouts)):\n",
    "                if len(self.dropouts)>1:\n",
    "                    x = fcc_drop(x, training)\n",
    "                x = fcc_dense(x)\n",
    "                if self.batch_norm:\n",
    "                    x = self.fcc_batch_norms[i](x, training)\n",
    "                x = tf.nn.relu(x)\n",
    "                \n",
    "        ####\n",
    "        output_1 = self.output_1(x)\n",
    "#         output_2 = self.output_2(x)\n",
    "#         output_total = self.output_total([output_1, output_2])\n",
    "        ####\n",
    "        \n",
    "        return output_1\n",
    "    \n",
    "   \n",
    "    def stackDropout(self):\n",
    "        drops = []\n",
    "        for rate in self.dropouts:\n",
    "            d = layers.Dropout(rate)\n",
    "            drops.append(d)\n",
    "        return drops\n",
    "        \n",
    "        \n",
    "    def stackDense(self):\n",
    "        denses, batch_norms = [], []\n",
    "        for units in self.features_dim:\n",
    "            dense_ = layers.Dense(units,\n",
    "                                   activation=None,\n",
    "                                   kernel_constraint=constraints.MaxNorm(self.max_norm),\n",
    "                                   bias_constraint=constraints.MaxNorm(self.max_norm),\n",
    "                                   kernel_regularizer=regularizers.l2(self.l2_reg), \n",
    "                                   bias_regularizer=regularizers.l2(self.l2_reg))\n",
    "            batch_norms.append(layers.BatchNormalization())\n",
    "            denses.append(dense_)\n",
    "        return denses, batch_norms\n",
    "    \n",
    "    \n",
    "\n",
    "class PulmonarFibrosisClassicModel(models.Model):\n",
    "    \n",
    "    def __init__(self, img_dim, features_dim, tabular_dim, dropouts, l2_reg, max_norm, batch_norm, path_img_model,\n",
    "                 learning_rate, clipvalue, quantiles, lambda_factor, beta_factor, \n",
    "                 first_epoch_learning_rate_epoch_decay, constant_learning_rate_epoch_decay, **kwargs):\n",
    "        super(PulmonarFibrosisClassicModel, self).__init__(**kwargs, name='PulmonarFibrosisClassicModel')\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        self.img_dim = img_dim\n",
    "        self.tabular_dim = tabular_dim\n",
    "        self.features_dim = features_dim\n",
    "        self.dropouts = dropouts\n",
    "        self.l2_reg = l2_reg\n",
    "        self.max_norm = max_norm\n",
    "        self.batch_norm = batch_norm\n",
    "        self.path_img_model = path_img_model\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.first_epoch_learning_rate_epoch_decay = first_epoch_learning_rate_epoch_decay\n",
    "        self.constant_learning_rate_epoch_decay = constant_learning_rate_epoch_decay \n",
    "        self.clipvalue = clipvalue\n",
    "        self.quantiles = tf.constant(quantiles, dtype=tf.float32)\n",
    "        self.lambda_factor = lambda_factor \n",
    "        self.beta_factor = beta_factor\n",
    "        \n",
    "        self.buildModel()\n",
    "        self.compile()\n",
    "        \n",
    "        \n",
    "    def compile(self):\n",
    "        super(PulmonarFibrosisClassicModel, self).compile()\n",
    "        \n",
    "        self.optimizer = optimizers.Adam(learning_rate=self.learning_rate, \n",
    "                                         clipvalue=self.clipvalue)\n",
    "#         self.optimizer=optimizers.Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "#                                       epsilon=None, decay=0.01, amsgrad=False, clipvalue=self.clipvalue)\n",
    "        \n",
    "        self.qloss = quantileLoss\n",
    "        self.scoreloss = customLossFunction\n",
    "        self.metric = [tf.keras.losses.MeanAbsoluteError(name='mae')]\n",
    "        \n",
    "#         self.compile(self.optimizer, self.qloss)\n",
    "    \n",
    "    def buildModel(self):\n",
    "        self.backbone_model = BackBoneModel(img_dim=self.img_dim, tabular_dim=self.tabular_dim,\n",
    "                                            features_dim=self.features_dim, \n",
    "                                            dropouts=self.dropouts,\n",
    "                                            l2_reg=self.l2_reg, batch_norm=self.batch_norm, \n",
    "                                            max_norm=self.max_norm,\n",
    "                                            path_img_model=self.path_img_model) \n",
    "    \n",
    "    def learningRateDecay(self, epoch):\n",
    "        if epoch == 0:\n",
    "            self.optimizer.learning_rate = self.optimizer.learning_rate * self.first_epoch_learning_rate_epoch_decay\n",
    "        else:\n",
    "            self.optimizer.learning_rate = self.optimizer.learning_rate * self.constant_learning_rate_epoch_decay\n",
    "            \n",
    "    @tf.function    \n",
    "    def train_step(self, backbone_inputs, target):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.backbone_model(backbone_inputs, training=True)\n",
    "            y_pred=tf.cast(y_pred, dtype=tf.float32)\n",
    "            target=tf.cast(target, dtype=tf.float32)\n",
    "            loss_1 = self.qloss(self.quantiles, \n",
    "                                unscale(target, mean_fvc, std_fvc), \n",
    "                                unscale(y_pred, mean_fvc, std_fvc))\n",
    "\n",
    "            loss_2 = self.scoreloss(unscale(target, mean_fvc, std_fvc), \n",
    "                                    unscale(y_pred[:, 1], mean_fvc, std_fvc),  \n",
    "                                    std=unscale(y_pred[:, 2], mean_fvc, std_fvc) -\n",
    "                                        unscale(y_pred[:, 0], mean_fvc, std_fvc))\n",
    "\n",
    "            loss = ((loss_1 * self.lambda_factor) + (loss_2 * (1-self.lambda_factor)))\n",
    "\n",
    "            mae = self.metric[0](y_true=target, y_pred=y_pred[:, 1])\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        return loss, loss_1, loss_2, mae\n",
    "\n",
    "    \n",
    "    def fitModel(self, X_train, X_val=None, epochs=1):\n",
    "        history = {}\n",
    "        history['loss'], history['val_loss'], history['metric'], history['val_metric'] = [], [], [], []\n",
    "        history['val_Metrict3Timesteps'] = []\n",
    "        \n",
    "        total_loss, total_metric1, total_metric2, total_metric3 = 0, 0, 0, 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            len_X_val = 0 if X_val is None else len(X_val)\n",
    "            len_X_train = len(X_train)\n",
    "            pbar = tf.keras.utils.Progbar(len_X_train + len_X_val)\n",
    "            \n",
    "            for num_batch, batch in enumerate(X_train):\n",
    "                img_features, tabular_features, target = batch\n",
    "                backbone_inputs = (img_features, tabular_features)\n",
    "                \n",
    "                loss, loss_1, loss_2, mae = self.train_step(backbone_inputs, target)\n",
    "                \n",
    "                pbar.update(num_batch + 1, values=[('Loss', loss)] + \\\n",
    "                                                  [('qloss', loss_1)]  + \\\n",
    "                                                  [('Metric', loss_2)] + \\\n",
    "                                                  [('mae', mae)]) \n",
    "                \n",
    "                total_loss += loss\n",
    "                total_metric1 += loss_1\n",
    "                total_metric2 += loss_2\n",
    "                total_metric3 += mae\n",
    "                \n",
    "            history['loss'].append(total_loss)\n",
    "            history['metric'].append(total_metric2)\n",
    "            \n",
    "            # Validation\n",
    "            if X_val:\n",
    "                val_total_loss, val_total_metric, val_total_metric2 = 0, 0, 0\n",
    "                for num_batch, batch in enumerate(X_val):\n",
    "                    img_features, tabular_features, val_target = batch\n",
    "                    backbone_inputs = (img_features, tabular_features)\n",
    "                    \n",
    "                    y_val_pred = self.backbone_model(backbone_inputs, training=False)\n",
    "                    \n",
    "                    val_loss_1 = self.qloss(self.quantiles, \n",
    "                                        unscale(val_target, mean_fvc, std_fvc), \n",
    "                                        unscale(y_val_pred, mean_fvc, std_fvc))\n",
    "\n",
    "                    val_loss_2 = self.scoreloss(unscale(val_target, mean_fvc, std_fvc),\n",
    "                                             unscale(y_val_pred[:, 1], mean_fvc, std_fvc),  \n",
    "                                             std=unscale(y_val_pred[:, 2], mean_fvc, std_fvc) -\n",
    "                                                 unscale(y_val_pred[:, 0], mean_fvc, std_fvc))\n",
    "\n",
    "                    val_loss = ((val_loss_1 * self.lambda_factor) + (val_loss_2 * (1-self.lambda_factor)))\n",
    "                    \n",
    "                    val_mae = self.metric[0](y_true=val_target, y_pred=y_val_pred[:, 1])\n",
    "                    \n",
    "                    pbar.update(len_X_train + num_batch + 1, values=[('val_Loss', val_loss)] + \\\n",
    "                                                                    [('val_qloss', val_loss_1)]  + \\\n",
    "                                                                    [('val_Metric', val_loss_2)] + \\\n",
    "                                                                    [('val_mae', val_mae)]) \n",
    "\n",
    "                    val_total_loss += val_loss\n",
    "                    val_total_metric += val_loss_1\n",
    "                    \n",
    "                val_total_loss  /= float(len_X_val)\n",
    "                val_total_metric /= float(len_X_val)\n",
    "                history['val_loss'].append(val_total_loss)\n",
    "                history['val_metric'].append(val_total_metric)\n",
    "                \n",
    "            self.learningRateDecay(epoch)\n",
    "            X_train.on_epoch_end() \n",
    "            if X_val:\n",
    "                X_val.on_epoch_end()\n",
    "            print(' ({:.0f} sec)\\n'.format( time.time() - start))\n",
    "        return history\n",
    "            \n",
    "            \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = dict(\n",
    "    img_dim=128, \n",
    "    tabular_dim=64,\n",
    "    features_dim=[32, 16], \n",
    "    dropouts=[0.3, 0.2],\n",
    "    l2_reg=1e-4, \n",
    "    max_norm=0.1,\n",
    "    batch_norm=False,\n",
    "    path_img_model=path_models + 'customModel',\n",
    "    \n",
    "    learning_rate=2e-3,\n",
    "    first_epoch_learning_rate_epoch_decay=0.9,\n",
    "    constant_learning_rate_epoch_decay=0.9,\n",
    "    \n",
    "    clipvalue=0.5,\n",
    "    lambda_factor=0.8,\n",
    "    beta_factor=0.6,\n",
    "    quantiles=[0.2, 0.5, 0.8]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 150405.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Fold: 1\n",
      "Train patients: 150, Test patients: 26\n",
      "Train rows: 1167, Test rows: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 762s 558ms/step - Loss: 172.4527 - qloss: 213.5752 - Metric: 7.9629 - mae: 0.6068 - val_Loss: 80.8077 - val_qloss: 99.1684 - val_Metric: 7.3648 - val_mae: 0.3024\n",
      " (762 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 99.4285 - qloss: 122.4291 - Metric: 7.4260 - mae: 0.3586 - val_Loss: 99.5388 - val_qloss: 122.5698 - val_Metric: 7.4147 - val_mae: 0.3635\n",
      " (757 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 92.2979 - qloss: 113.5333 - Metric: 7.3566 - mae: 0.3384 - val_Loss: 78.4923 - val_qloss: 96.2948 - val_Metric: 7.2825 - val_mae: 0.2860\n",
      " (757 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 760s 556ms/step - Loss: 90.2959 - qloss: 111.0362 - Metric: 7.3344 - mae: 0.3271 - val_Loss: 81.5855 - val_qloss: 100.1576 - val_Metric: 7.2971 - val_mae: 0.2978\n",
      " (760 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 763s 558ms/step - Loss: 90.5622 - qloss: 111.3701 - Metric: 7.3311 - mae: 0.3283 - val_Loss: 72.0715 - val_qloss: 88.2839 - val_Metric: 7.2221 - val_mae: 0.2538\n",
      " (763 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 759s 556ms/step - Loss: 87.0913 - qloss: 107.0358 - Metric: 7.3135 - mae: 0.3161 - val_Loss: 70.0634 - val_qloss: 85.7832 - val_Metric: 7.1843 - val_mae: 0.2461\n",
      " (759 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      "1366/1366 [==============================] - 758s 555ms/step - Loss: 83.5527 - qloss: 102.6264 - Metric: 7.2578 - mae: 0.3020 - val_Loss: 69.5517 - val_qloss: 85.1623 - val_Metric: 7.1094 - val_mae: 0.2410\n",
      " (758 sec)\n",
      "\n",
      "Epoch [8/8]\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 81.8021 - qloss: 100.4423 - Metric: 7.2411 - mae: 0.2963 - val_Loss: 75.3521 - val_qloss: 92.3917 - val_Metric: 7.1935 - val_mae: 0.2744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 75704.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (757 sec)\n",
      "\n",
      "Num Fold: 2\n",
      "Train patients: 151, Test patients: 25\n",
      "Train rows: 1174, Test rows: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 161.3140 - qloss: 199.6653 - Metric: 7.9086 - mae: 0.5616 - val_Loss: 112.4913 - val_qloss: 138.7033 - val_Metric: 7.6432 - val_mae: 0.4309\n",
      " (757 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 97.7817 - qloss: 120.3747 - Metric: 7.4098 - mae: 0.3534 - val_Loss: 96.8318 - val_qloss: 119.1808 - val_Metric: 7.4354 - val_mae: 0.3722\n",
      " (755 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 756s 553ms/step - Loss: 92.4979 - qloss: 113.7834 - Metric: 7.3559 - mae: 0.3352 - val_Loss: 131.7804 - val_qloss: 162.8412 - val_Metric: 7.5375 - val_mae: 0.4697: 96.1474 - qloss: 118.3337 - Metric - ETA: 10:45 - Loss: 96.1289 - qloss: 118 - ETA: 10:15 - Loss: 91.9480 - qloss: 113.0924 - Metric: 7.3702 - - ETA: 10:11 - Loss: 92.1412 - qloss: 11\n",
      " (756 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 755s 552ms/step - Loss: 86.7510 - qloss: 106.6168 - Metric: 7.2878 - mae: 0.3126 - val_Loss: 126.7197 - val_qloss: 156.4977 - val_Metric: 7.6072 - val_mae: 0.4588\n",
      " (755 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 85.3855 - qloss: 104.9085 - Metric: 7.2934 - mae: 0.3091 - val_Loss: 86.5681 - val_qloss: 106.3722 - val_Metric: 7.3513 - val_mae: 0.3284\n",
      " (755 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 81.6094 - qloss: 100.2011 - Metric: 7.2422 - mae: 0.2969 - val_Loss: 87.4302 - val_qloss: 107.4592 - val_Metric: 7.3140 - val_mae: 0.3197\n",
      " (755 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      "1366/1366 [==============================] - 755s 552ms/step - Loss: 82.4776 - qloss: 101.2852 - Metric: 7.2471 - mae: 0.2982 - val_Loss: 100.7145 - val_qloss: 124.0300 - val_Metric: 7.4525 - val_mae: 0.3808- qloss:\n",
      " (755 sec)\n",
      "\n",
      "Epoch [8/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 79.5538 - qloss: 97.6435 - Metric: 7.1957 - mae: 0.2857 - val_Loss: 85.8615 - val_qloss: 105.5063 - val_Metric: 7.2824 - val_mae: 0.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 151408.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 25073.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (755 sec)\n",
      "\n",
      "Num Fold: 3\n",
      "Train patients: 151, Test patients: 25\n",
      "Train rows: 1169, Test rows: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 194.1331 - qloss: 240.6044 - Metric: 8.2485 - mae: 0.6895 - val_Loss: 199.7362 - val_qloss: 247.6682 - val_Metric: 8.0080 - val_mae: 0.6969\n",
      " (755 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 116.2125 - qloss: 143.3726 - Metric: 7.5724 - mae: 0.4227 - val_Loss: 112.8002 - val_qloss: 139.1051 - val_Metric: 7.5805 - val_mae: 0.4329\n",
      " (753 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 754s 552ms/step - Loss: 97.5382 - qloss: 120.0657 - Metric: 7.4280 - mae: 0.3579 - val_Loss: 116.7670 - val_qloss: 144.0658 - val_Metric: 7.5720 - val_mae: 0.4337\n",
      " (754 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 753s 552ms/step - Loss: 94.7937 - qloss: 116.6436 - Metric: 7.3941 - mae: 0.3479 - val_Loss: 109.4215 - val_qloss: 134.8943 - val_Metric: 7.5297 - val_mae: 0.41372s - Loss: 94.7\n",
      " (753 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 753s 552ms/step - Loss: 89.9667 - qloss: 110.6216 - Metric: 7.3472 - mae: 0.3287 - val_Loss: 88.4789 - val_qloss: 108.7555 - val_Metric: 7.3724 - val_mae: 0.3365\n",
      " (753 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 754s 552ms/step - Loss: 90.5368 - qloss: 111.3359 - Metric: 7.3402 - mae: 0.3258 - val_Loss: 84.4615 - val_qloss: 103.7440 - val_Metric: 7.3314 - val_mae: 0.3023\n",
      " (754 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 83.1726 - qloss: 102.1461 - Metric: 7.2784 - mae: 0.3025 - val_Loss: 105.9932 - val_qloss: 130.6220 - val_Metric: 7.4783 - val_mae: 0.4045\n",
      " (753 sec)\n",
      "\n",
      "Epoch [8/8]\n",
      "1366/1366 [==============================] - 754s 552ms/step - Loss: 80.7334 - qloss: 99.1053 - Metric: 7.2467 - mae: 0.2957 - val_Loss: 89.8421 - val_qloss: 110.4697 - val_Metric: 7.3315 - val_mae: 0.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 147151.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 25085.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (754 sec)\n",
      "\n",
      "Num Fold: 4\n",
      "Train patients: 151, Test patients: 25\n",
      "Train rows: 1170, Test rows: 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 755s 552ms/step - Loss: 140.5281 - qloss: 173.7127 - Metric: 7.7898 - mae: 0.5014 - val_Loss: 105.3917 - val_qloss: 129.8321 - val_Metric: 7.6301 - val_mae: 0.4007\n",
      " (755 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 102.3819 - qloss: 126.1167 - Metric: 7.4422 - mae: 0.3670 - val_Loss: 68.2483 - val_qloss: 83.5227 - val_Metric: 7.1509 - val_mae: 0.23971:07 - Loss: 102.3819 - qloss: 126.1167 - Metric: 7.4422 - mae: 0.3670 - val_Loss: 62.9488 - val_qloss: 76.9086 - val_Metric: 7.1099 - val_mae: 0. - ETA: 1:05\n",
      " (753 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 95.6229 - qloss: 117.6829 - Metric: 7.3826 - mae: 0.3462 - val_Loss: 73.3627 - val_qloss: 89.8819 - val_Metric: 7.2861 - val_mae: 0.2504\n",
      " (753 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 754s 552ms/step - Loss: 89.3853 - qloss: 109.8993 - Metric: 7.3294 - mae: 0.3221 - val_Loss: 71.7213 - val_qloss: 87.8579 - val_Metric: 7.1748 - val_mae: 0.2579\n",
      " (754 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 86.5378 - qloss: 106.3546 - Metric: 7.2706 - mae: 0.3122 - val_Loss: 74.3805 - val_qloss: 91.1845 - val_Metric: 7.1645 - val_mae: 0.2652\n",
      " (753 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 83.3350 - qloss: 102.3589 - Metric: 7.2397 - mae: 0.3020 - val_Loss: 60.5652 - val_qloss: 73.9409 - val_Metric: 7.0627 - val_mae: 0.19691s - Loss: 83.3350 - qloss: 102.3589 - Metric: 7.2397 - mae: 0.3020 - val_Loss: 61.9287\n",
      " (753 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 82.0608 - qloss: 100.7768 - Metric: 7.1962 - mae: 0.2948 - val_Loss: 76.4663 - val_qloss: 93.7934 - val_Metric: 7.1579 - val_mae: 0.2814\n",
      " (753 sec)\n",
      "\n",
      "Epoch [8/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 79.3858 - qloss: 97.4349 - Metric: 7.1895 - mae: 0.2849 - val_Loss: 66.9669 - val_qloss: 81.9470 - val_Metric: 7.0466 - val_mae: 0.2467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 75541.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (753 sec)\n",
      "\n",
      "Num Fold: 5\n",
      "Train patients: 151, Test patients: 25\n",
      "Train rows: 1172, Test rows: 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 754s 552ms/step - Loss: 158.5428 - qloss: 196.2137 - Metric: 7.8601 - mae: 0.5554 - val_Loss: 81.3484 - val_qloss: 99.8610 - val_Metric: 7.2980 - val_mae: 0.3121\n",
      " (754 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 752s 551ms/step - Loss: 104.6644 - qloss: 128.9607 - Metric: 7.4796 - mae: 0.3827 - val_Loss: 70.1961 - val_qloss: 85.9362 - val_Metric: 7.2357 - val_mae: 0.2466\n",
      " (752 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 752s 550ms/step - Loss: 97.3502 - qloss: 119.8398 - Metric: 7.3916 - mae: 0.3497 - val_Loss: 75.5133 - val_qloss: 92.5808 - val_Metric: 7.2431 - val_mae: 0.2789\n",
      " (752 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 752s 551ms/step - Loss: 93.8667 - qloss: 115.4945 - Metric: 7.3552 - mae: 0.3384 - val_Loss: 83.7936 - val_qloss: 102.9158 - val_Metric: 7.3048 - val_mae: 0.2953\n",
      " (752 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 752s 550ms/step - Loss: 89.9204 - qloss: 110.5673 - Metric: 7.3322 - mae: 0.3276 - val_Loss: 66.0468 - val_qloss: 80.7673 - val_Metric: 7.1645 - val_mae: 0.2441\n",
      " (752 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 91.0269 - qloss: 111.9499 - Metric: 7.3348 - mae: 0.3314 - val_Loss: 61.8857 - val_qloss: 75.5713 - val_Metric: 7.1432 - val_mae: 0.2127\n",
      " (753 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      "1366/1366 [==============================] - 752s 551ms/step - Loss: 88.4787 - qloss: 108.7713 - Metric: 7.3088 - mae: 0.3206 - val_Loss: 80.5201 - val_qloss: 98.8235 - val_Metric: 7.3063 - val_mae: 0.3090\n",
      " (752 sec)\n",
      "\n",
      "Epoch [8/8]\n",
      "1366/1366 [==============================] - 752s 551ms/step - Loss: 85.6702 - qloss: 105.2686 - Metric: 7.2766 - mae: 0.3119 - val_Loss: 54.8069 - val_qloss: 66.7590 - val_Metric: 6.9986 - val_mae: 0.1918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 151408.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (752 sec)\n",
      "\n",
      "Num Fold: 6\n",
      "Train patients: 151, Test patients: 25\n",
      "Train rows: 1171, Test rows: 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 147.3031 - qloss: 182.1557 - Metric: 7.8930 - mae: 0.5480 - val_Loss: 100.1857 - val_qloss: 123.3964 - val_Metric: 7.3431 - val_mae: 0.3470\n",
      " (757 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 755s 552ms/step - Loss: 99.5581 - qloss: 122.5873 - Metric: 7.4412 - mae: 0.3601 - val_Loss: 86.8084 - val_qloss: 106.6953 - val_Metric: 7.2608 - val_mae: 0.2979\n",
      " (755 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 90.1638 - qloss: 110.8656 - Metric: 7.3566 - mae: 0.3296 - val_Loss: 71.7450 - val_qloss: 87.8814 - val_Metric: 7.1993 - val_mae: 0.2572\n",
      " (755 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 92.6397 - qloss: 113.9627 - Metric: 7.3472 - mae: 0.3345 - val_Loss: 71.9376 - val_qloss: 88.1287 - val_Metric: 7.1732 - val_mae: 0.2515- Loss: 91.161 - ETA: 4:06 - Loss: 90.1078 - qloss: 110.8035 - Metric: 7.3249 - mae:  \n",
      " (757 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 756s 553ms/step - Loss: 90.3456 - qloss: 111.0929 - Metric: 7.3566 - mae: 0.3284 - val_Loss: 71.2550 - val_qloss: 87.2544 - val_Metric: 7.2574 - val_mae: 0.2546\n",
      " (756 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 85.8012 - qloss: 105.4261 - Metric: 7.3017 - mae: 0.3092 - val_Loss: 77.2919 - val_qloss: 94.7988 - val_Metric: 7.2641 - val_mae: 0.2955\n",
      " (755 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      "1366/1366 [==============================] - 756s 553ms/step - Loss: 84.1655 - qloss: 103.3877 - Metric: 7.2773 - mae: 0.3048 - val_Loss: 57.2774 - val_qloss: 69.8406 - val_Metric: 7.0247 - val_mae: 0.2066\n",
      " (756 sec)\n",
      "\n",
      "Epoch [8/8]\n",
      "1366/1366 [==============================] - 755s 553ms/step - Loss: 80.9270 - qloss: 99.3527 - Metric: 7.2242 - mae: 0.2921 - val_Loss: 53.1775 - val_qloss: 64.7266 - val_Metric: 6.9809 - val_mae: 0.1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 147942.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (755 sec)\n",
      "\n",
      "Num Fold: 7\n",
      "Train patients: 151, Test patients: 25\n",
      "Train rows: 1173, Test rows: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8]\n",
      "WARNING:tensorflow:Layer BackBoneModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1366/1366 [==============================] - 757s 554ms/step - Loss: 153.4107 - qloss: 189.7612 - Metric: 8.0090 - mae: 0.5292 - val_Loss: 78.5446 - val_qloss: 96.3529 - val_Metric: 7.3113 - val_mae: 0.2797\n",
      " (757 sec)\n",
      "\n",
      "Epoch [2/8]\n",
      "1366/1366 [==============================] - 752s 551ms/step - Loss: 99.5392 - qloss: 122.5698 - Metric: 7.4166 - mae: 0.3564 - val_Loss: 74.9878 - val_qloss: 91.9199 - val_Metric: 7.2597 - val_mae: 0.2736\n",
      " (752 sec)\n",
      "\n",
      "Epoch [3/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 97.1469 - qloss: 119.5827 - Metric: 7.4042 - mae: 0.3529 - val_Loss: 82.3882 - val_qloss: 101.1508 - val_Metric: 7.3378 - val_mae: 0.2950\n",
      " (753 sec)\n",
      "\n",
      "Epoch [4/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 96.5876 - qloss: 118.8858 - Metric: 7.3951 - mae: 0.3481 - val_Loss: 83.9833 - val_qloss: 103.1519 - val_Metric: 7.3092 - val_mae: 0.3109\n",
      " (753 sec)\n",
      "\n",
      "Epoch [5/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 90.0345 - qloss: 110.7115 - Metric: 7.3267 - mae: 0.3245 - val_Loss: 91.3373 - val_qloss: 112.3318 - val_Metric: 7.3595 - val_mae: 0.3379\n",
      " (753 sec)\n",
      "\n",
      "Epoch [6/8]\n",
      "1366/1366 [==============================] - 753s 551ms/step - Loss: 90.1457 - qloss: 110.8516 - Metric: 7.3219 - mae: 0.3269 - val_Loss: 72.9382 - val_qloss: 89.3739 - val_Metric: 7.1954 - val_mae: 0.2691\n",
      " (753 sec)\n",
      "\n",
      "Epoch [7/8]\n",
      " 582/1366 [===========>..................] - ETA: 7:36 - Loss: 89.6860 - qloss: 110.2775 - Metric: 7.3204 - mae: 0.3247- ETA: 8:53 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e96d6aff4583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mX_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_val_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     )\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-fea1d8bc7c18>\u001b[0m in \u001b[0;36mfitModel\u001b[1;34m(self, X_train, X_val, epochs)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_X_train\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen_X_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m                 \u001b[0mimg_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtabular_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0mbackbone_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtabular_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m       \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    484\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m       \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f70b199a030d>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                                   \u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size_load\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size_load\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                                                   numpy=True) \n\u001b[1;32m---> 51\u001b[1;33m                               for patient in patient_ids]\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mlist_scan_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessRawScans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatient_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f70b199a030d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                                   \u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size_load\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size_load\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                                                   numpy=True) \n\u001b[1;32m---> 51\u001b[1;33m                               for patient in patient_ids]\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mlist_scan_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessRawScans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatient_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\PROYECTOS\\OSIC Pulmonary - 2020\\02_Scripts\\Utils\\utils.py\u001b[0m in \u001b[0;36mdecodePatientImages\u001b[1;34m(patient, dict_patients_masks_paths, image_size, numpy)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_patients_masks_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindowImageNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1_000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#min_bound=-1_000, max_bound=400\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         img_resized = tf.convert_to_tensor([tf.image.resize(tf.expand_dims(img, axis=2), image_size) for img in imgs], \n",
      "\u001b[1;32mG:\\PROYECTOS\\OSIC Pulmonary - 2020\\02_Scripts\\Utils\\utils.py\u001b[0m in \u001b[0;36mwindowImageNorm\u001b[1;34m(image, min_bound, max_bound)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwindowImageNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1_000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_bound\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_bound\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_size_load=(260, 260, 1)\n",
    "img_size_crop=(220, 220, 1)\n",
    "num_frames_batch = 32\n",
    "train_alpha = 0.7\n",
    "val_alpha = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 7, random_state = 12, shuffle = True)\n",
    "list_models, list_history = [], []\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(skf.split(unique_train_patients, \n",
    "                                                              np.zeros(unique_train_patients.shape[0]))):\n",
    "\n",
    "    x_train_patients = list(unique_train_patients[train_index])\n",
    "    x_val_patients = list(unique_train_patients[val_index])\n",
    "    \n",
    "    print(f'Num Fold: {num_fold + 1}')\n",
    "    print(f'Train patients: {len(x_train_patients)}, Test patients: {len(x_val_patients)}') \n",
    "    \n",
    "    df_train_model = buildDataSet(x_train_patients,\n",
    "                             dict_ini_features=dict_patients_train_ini_features, \n",
    "                             dict_seq_cumweeks=dict_train_sequence_cumweeks, \n",
    "                             training=True, \n",
    "                             predictions=None)\n",
    "    \n",
    "    df_val_model = buildDataSet(x_val_patients,\n",
    "                             dict_ini_features=dict_patients_train_ini_features, \n",
    "                             dict_seq_cumweeks=dict_train_sequence_cumweeks, \n",
    "                             training=True, \n",
    "                             predictions=None)\n",
    "    \n",
    "    print(f'Train rows: {df_train_model.shape[0]}, Test rows: {df_val_model.shape[0]}')\n",
    "\n",
    "    X_train_generator = ForecastTabularImgDataGenerator(raw_scans=False, training=True, \n",
    "                                                        patients=x_train_patients, df_tabular=df_train_model,\n",
    "                                                        batch_size=batch_size, num_frames_batch=num_frames_batch, \n",
    "                                                        alpha=train_alpha, random_window=True, center_crop=True,\n",
    "                                                        img_size_load=img_size_load, img_size_crop=img_size_crop,\n",
    "                                                        dict_ini_features=dict_patients_train_ini_features, \n",
    "                                                        dict_patients_masks_paths=dict_train_patients_masks_paths,\n",
    "                                                        dict_raw_scans_paths=None)\n",
    "\n",
    "    X_val_generator = ForecastTabularImgDataGenerator(raw_scans=False, training=False, \n",
    "                                                      patients=x_val_patients, df_tabular=df_val_model,\n",
    "                                                      batch_size=1, num_frames_batch=num_frames_batch, \n",
    "                                                      alpha=val_alpha, random_window=True, center_crop=True,\n",
    "                                                      img_size_load=img_size_load, img_size_crop=img_size_crop,\n",
    "                                                      dict_ini_features=dict_patients_train_ini_features, \n",
    "                                                      dict_patients_masks_paths=dict_train_patients_masks_paths,\n",
    "                                                      dict_raw_scans_paths=None)\n",
    "\n",
    "\n",
    "    model = PulmonarFibrosisClassicModel(**model_inputs)\n",
    "    \n",
    "    \n",
    "    history = model.fitModel(\n",
    "        X_train=X_train_generator,\n",
    "        X_val=X_val_generator,\n",
    "        epochs=8\n",
    "    )\n",
    "    \n",
    "    list_models.append(model)\n",
    "    list_history.append(history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_num_visits = {}\n",
    "# for patient in dict_train_sequence_fvc:\n",
    "#     num_visits = len(dict_train_sequence_fvc[patient])\n",
    "#     if num_visits not in dict_num_visits:\n",
    "#         dict_num_visits[num_visits] = []\n",
    "#     else:\n",
    "#         dict_num_visits[num_visits].append(np.mean(unscale(np.array(dict_train_sequence_fvc[patient]), mean_fvc, std_fvc)))\n",
    "        \n",
    "# print(dict_num_visits)\n",
    "# print('==='*20)\n",
    "# for i in dict_num_visits:\n",
    "#     print(i)\n",
    "#     print(np.mean(dict_num_visits[i]))\n",
    "#     print('==='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.00115 86.966705\n"
     ]
    }
   ],
   "source": [
    "val_loss = np.mean([history['val_loss'][-1] for history in list_history])\n",
    "val_metric = np.mean([(history['val_metric'][-1]) for history in list_history])\n",
    "# val_metric_last3 = np.mean([(history['val_Metrict3Timesteps'][-1]) for history in list_history])\n",
    "\n",
    "print(val_loss, val_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### History models\n",
    "# 1. val_loss - 0.14948401 & val_metric = 7.581691 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0\n",
    "# 2. 1.7004111 7.6539536 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=True & dim=128\n",
    "# 3. 1.9819709 7.499232 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.75 & resnet=False & dim=256 & visuallatt\n",
    "# 4. 1.6195476 7.4542327 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=custom & dim=128\n",
    "# 5. (7.30) 1.5714737 7.306921 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=custom & dim=128 & lrdecay=0.9\n",
    "# 6. (Best - 7.00) | 1.5109245 7.061371 quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=custom & dim=128 & inidecay=0.5 & lrdecay=0.9\n",
    "# 7. 1.4737307 6.9873514 7.0969524 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & beta_factor=0.6 & resnet=custom & dim=128 & inidecay=0.9 & lrdecay=0.9\n",
    "# 8. 1.4489578 6.906362 6.972941 | Add kind patient feature\n",
    "# 9. 1.425578 6.822274 6.857718 | Add kind and remove dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "\n",
    "def plotAttention(images, list_weeks_elapsed, result, attention_plot, alpha=0.7, max_imgs=False):\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        if max_imgs:\n",
    "            temp_image = np.max(images, axis=0)\n",
    "        len_result = len(result)\n",
    "        for i in range(len_result):\n",
    "            if not max_imgs:\n",
    "                temp_image = images[i]\n",
    "            temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "            if len_result >= 2:\n",
    "                ax = fig.add_subplot(len_result//2, len_result//2, i+1)\n",
    "            else:\n",
    "                ax = fig.add_subplot(1, 1, 1)\n",
    "            ax.set_title(f'Weeks: {list_weeks_elapsed[i]} - Pred: {int(result[i])}')\n",
    "            img = ax.imshow(temp_image, cmap=plt.cm.bone)\n",
    "            ax.imshow(temp_att, cmap='gray', alpha=alpha, extent=img.get_extent())\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "X_generator = SequenceToSequenceDataGenerator(raw_scans=False, training=False, patients=x_val_patients,\n",
    "                                                  batch_size=1, num_frames_batch=32, \n",
    "                                                  alpha=val_alpha, random_window=True, center_crop=True,\n",
    "                                                  img_size_load=img_size_load, img_size_crop=img_size_crop,\n",
    "                                                  dict_ini_features=dict_patients_train_ini_features, \n",
    "                                                  dict_patients_masks_paths=dict_train_patients_masks_paths,\n",
    "                                                  dict_raw_scans_paths=None)\n",
    "\n",
    "\n",
    "patient = np.random.choice(unique_train_patients)\n",
    "print(f'Patient: {patient}')\n",
    "# # # patient = 'ID00267637202270790561585'\n",
    "batch = X_train_generator.getOnePatient(patient)\n",
    "list_weeks_elapsed = list(dict_train_sequence_weekssincelastvisit[patient])\n",
    "list_weeks_cum = list(dict_train_sequence_cumweeks[patient])\n",
    "result, confidences, attention_plot = model.predictEvaluateModel(X_generator=X_generator,\n",
    "                                                      batch=None,\n",
    "                                                      patient=patient, \n",
    "                                                      list_weeks_elapsed=list_weeks_elapsed, \n",
    "                                                      list_weeks_since_firstvisit=list_weeks_cum,\n",
    "                                                      initial_fvc=np.asarray([dict_patients_train_ini_features[patient]['FVC']]))\n",
    "                                                      # initial_fvc=np.asarray([scale(500, mean_fvc, std_fvc)]))\n",
    "\n",
    "patient_imgs = batch[0]\n",
    "plotAttention(patient_imgs[0].squeeze(), list_weeks_elapsed, \n",
    "              unscale(result.numpy(), mean_fvc, std_fvc), attention_plot, alpha=0.8, max_imgs=True)\n",
    "plotSequencePrediction(unscale(result.numpy(), mean_fvc, std_fvc),\n",
    "                       unscale(np.array(dict_train_sequence_fvc[patient]), mean_fvc, std_fvc), \n",
    "                       list_weeks_elapsed)\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baselines\n",
    "\n",
    "- 1. **Mean FVC** from training to all predictions.\n",
    "- 2. **Initial FVC per Patient** from each patient we take initial fvc and predict it over all timesteps.\n",
    "- 3. **Last timestep** (Although this is a good baseline for general Sequence-to-Sequence purposes it is not for this case because we want to focus on the prognosis availability of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1- Mean FVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Predict always the train FVC mean\n",
    "\n",
    "test_patients = list(df_test['Patient'].unique())\n",
    "dict_predictions = {\n",
    "    'Patient': [],\n",
    "    'target': [],\n",
    "    'prediction': []\n",
    "}\n",
    "\n",
    "global_train_mean_fvc = df_train.FVC.mean()\n",
    "\n",
    "for i, patient in enumerate(test_patients):\n",
    "    subset = df_train[df_train['Patient']==patient]\n",
    "    list_weeks_elapsed = dict_train_sequence_weekssincelastvisit[patient]\n",
    "    list_fvc_sequence = np.array(dict_train_sequence_fvc[patient])\n",
    "    \n",
    "    list_pred = list_fvc_sequence.copy()\n",
    "    list_pred[:] = global_train_mean_fvc\n",
    "    \n",
    "    dict_predictions['Patient'].append(patient)\n",
    "    dict_predictions['target'].append(unscale(list_fvc_sequence, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['prediction'].append(unscale(list_pred, mean_fvc, std_fvc).astype(int))\n",
    "\n",
    "df_base = pd.DataFrame({'Target': dict_predictions['target'], \n",
    "                        'Pred': dict_predictions['prediction']}, \n",
    "                       index=[dict_predictions['Patient']])\n",
    "\n",
    "df_base['mse'] = df_base.apply(lambda x: np.mean((abs(np.array(x['Target']) - x['Pred'])**2)) ,axis=1)\n",
    "df_base['rmse'] = df_base.apply(lambda x: np.sqrt(np.mean(abs(np.array(x['Target']) - x['Pred']))) ,axis=1)\n",
    "df_base['mape'] = df_base.apply(lambda x: 100.0 * np.mean(abs((np.array(x['Target']) - x['Pred'])/np.array(x['Target']))),axis=1)\n",
    "df_base['metric'] = df_base.apply(lambda x: np.mean([customLossFunction(x['Target'][i], \n",
    "                                                                        x['Pred'][i]).numpy() \n",
    "                                                     for i in range(len(x['Pred']))]),\n",
    "                                  axis=1)\n",
    "print('==='*20)\n",
    "print('Metrics: ')\n",
    "print(f\"MSE: {np.mean(df_base['mse'])} - RMSE: {np.mean(df_base['rmse'])} - MAPE: {np.mean(df_base['mape'])}\")\n",
    "print(f\"CustomMetric: {np.mean(df_base['metric'])}\")\n",
    "\n",
    "df_base\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2 - Initial FVC per Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Predict always last Timestep\n",
    "\n",
    "test_patients = list(df_test['Patient'].unique())\n",
    "dict_predictions = {\n",
    "    'Patient': [],\n",
    "    'target': [],\n",
    "    'prediction': []\n",
    "}\n",
    "\n",
    "for patient in test_patients:\n",
    "    subset = df_train[df_train['Patient']==patient]\n",
    "    list_weeks_elapsed = dict_train_sequence_weekssincelastvisit[patient]\n",
    "    list_fvc_sequence = np.array(dict_train_sequence_fvc[patient])\n",
    "    \n",
    "    predictions = np.empty(list_fvc_sequence.shape)\n",
    "    predictions[0:] = dict_patients_train_ini_features[patient]['FVC']\n",
    "    \n",
    "    dict_predictions['Patient'].append(patient)\n",
    "    dict_predictions['target'].append(unscale(list_fvc_sequence, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['prediction'].append(unscale(predictions, mean_fvc, std_fvc).astype(int))\n",
    "    \n",
    "\n",
    "df_base = pd.DataFrame({'Target': dict_predictions['target'], \n",
    "                        'Pred': dict_predictions['prediction']}, \n",
    "                       index=[dict_predictions['Patient']])\n",
    "\n",
    "df_base['mse'] = df_base.apply(lambda x: np.mean((abs(np.array(x['Target']) - x['Pred'])**2)) ,axis=1)\n",
    "df_base['rmse'] = df_base.apply(lambda x: np.sqrt(np.mean(abs(np.array(x['Target']) - x['Pred']))) ,axis=1)\n",
    "df_base['mape'] = df_base.apply(lambda x: 100.0 * np.mean(abs((np.array(x['Target']) - x['Pred'])/np.array(x['Target']))),axis=1)\n",
    "df_base['metric'] = df_base.apply(lambda x: np.mean([customLossFunction(x['Target'][i], \n",
    "                                                                        x['Pred'][i]).numpy() \n",
    "                                                     for i in range(len(x['Pred']))]),\n",
    "                                  axis=1)\n",
    "print('==='*20)\n",
    "print('Metrics: ')\n",
    "print(f\"MSE: {np.mean(df_base['mse'])} - RMSE: {np.mean(df_base['rmse'])} - MAPE: {np.mean(df_base['mape'])}\")\n",
    "print(f\"CustomMetric: {np.mean(df_base['metric'])}\")\n",
    "df_base\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 3 - Last Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Predict always initial FVC\n",
    "\n",
    "test_patients = list(df_test['Patient'].unique())\n",
    "dict_predictions = {\n",
    "    'Patient': [],\n",
    "    'target': [],\n",
    "    'prediction': []\n",
    "}\n",
    "\n",
    "for patient in test_patients:\n",
    "    subset = df_train[df_train['Patient']==patient]\n",
    "    list_weeks_elapsed = dict_train_sequence_weekssincelastvisit[patient]\n",
    "    list_fvc_sequence = np.array(dict_train_sequence_fvc[patient])\n",
    "    \n",
    "    list_pred = np.empty(list_fvc_sequence.shape)\n",
    "    list_pred[0] = dict_patients_train_ini_features[patient]['FVC']\n",
    "    list_pred[1:] = list_fvc_sequence[0:-1]\n",
    "    \n",
    "    dict_predictions['Patient'].append(patient)\n",
    "    dict_predictions['target'].append(unscale(list_fvc_sequence, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['prediction'].append(unscale(list_pred, mean_fvc, std_fvc).astype(int))\n",
    "    \n",
    "\n",
    "df_base = pd.DataFrame({'Target': dict_predictions['target'], \n",
    "                        'Pred': dict_predictions['prediction']}, \n",
    "                       index=[dict_predictions['Patient']])\n",
    "\n",
    "df_base['mse'] = df_base.apply(lambda x: np.mean((abs(np.array(x['Target']) - x['Pred'])**2)) ,axis=1)\n",
    "df_base['rmse'] = df_base.apply(lambda x: np.sqrt(np.mean(abs(np.array(x['Target']) - x['Pred']))) ,axis=1)\n",
    "df_base['mape'] = df_base.apply(lambda x: 100.0 * np.mean(abs((np.array(x['Target']) - x['Pred'])/np.array(x['Target']))),axis=1)\n",
    "df_base['metric'] = df_base.apply(lambda x: np.mean([customLossFunction(x['Target'][i], \n",
    "                                                                        x['Pred'][i]).numpy() \n",
    "                                                     for i in range(len(x['Pred']))]),\n",
    "                                  axis=1)\n",
    "print('==='*20)\n",
    "print('Metrics: ')\n",
    "print(f\"MSE: {np.mean(df_base['mse'])} - RMSE: {np.mean(df_base['rmse'])} - MAPE: {np.mean(df_base['mape'])}\")\n",
    "print(f\"CustomMetric: {np.mean(df_base['metric'])}\")\n",
    "df_base\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Model Prediction\n",
    "\n",
    "test_patients = list(df_test['Patient'].unique())\n",
    "dict_predictions = {\n",
    "    'Patient': [],\n",
    "    'target': [],\n",
    "    'prediction': [],\n",
    "    'confidences' : []\n",
    "}\n",
    "\n",
    "for patient in test_patients:\n",
    "    subset = df_train[df_train['Patient']==patient]\n",
    "    list_weeks_elapsed = dict_train_sequence_weekssincelastvisit[patient]\n",
    "    list_weeks_cum = list(dict_train_sequence_cumweeks[patient])\n",
    "    list_fvc_sequence = np.array(dict_train_sequence_fvc[patient])\n",
    "    \n",
    "    result, stds, _ = model.predictEvaluateModel(X_generator=X_val_generator,\n",
    "                                                    patient=patient, \n",
    "                                                    list_weeks_elapsed=list_weeks_elapsed,\n",
    "                                                    list_weeks_since_firstvisit=list_weeks_cum,\n",
    "                                                    initial_fvc=[dict_patients_train_ini_features[patient]['FVC']])\n",
    "    \n",
    "    predictions, confidences = np.empty(len(list_fvc_sequence)+1), np.empty(len(list_fvc_sequence)+1)\n",
    "    targets = np.empty(len(list_fvc_sequence)+1)\n",
    "    predictions[0], confidences[0] = dict_patients_test_ini_features[patient]['FVC'], 100.0\n",
    "    targets[0] = dict_patients_test_ini_features[patient]['FVC']\n",
    "    predictions[1:] = result.numpy().flatten()\n",
    "    confidences[1:] = stds.numpy().flatten()#  * 100\n",
    "    targets[1:] = list_fvc_sequence\n",
    "    \n",
    "    dict_predictions['Patient'].append(patient)\n",
    "    dict_predictions['target'].append(unscale(targets, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['prediction'].append(unscale(predictions, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['confidences'].append(confidences)\n",
    "    \n",
    "    df_metrics = pd.DataFrame({'Target': dict_predictions['target'], \n",
    "                               'Pred': dict_predictions['prediction'],\n",
    "                               'Confidences' :dict_predictions['confidences']}, \n",
    "                                index=[dict_predictions['Patient']])\n",
    "    \n",
    "\n",
    "df_metrics['mse'] = df_metrics.apply(lambda x: np.mean((x['Target'] - x['Pred'])**2), axis=1)\n",
    "df_metrics['rmse'] = df_metrics.apply(lambda x: np.sqrt(np.mean(abs(x['Target'] - x['Pred']))) ,axis=1)\n",
    "df_metrics['mape'] = df_metrics.apply(lambda x: 100.0 * np.mean(abs((x['Target'] - x['Pred'])/x['Target'])),axis=1)\n",
    "df_metrics['metric'] = df_metrics.apply(lambda x: np.mean([customLossFunction(x['Target'][i], \n",
    "                                                                              x['Pred'][i],\n",
    "                                                                              std=x['Confidences'][i]).numpy() \n",
    "                                                           for i in range(len(x['Pred']))]),\n",
    "                                  axis=1)\n",
    "\n",
    "print('==='*20)\n",
    "print('Metrics: ')\n",
    "print(f\"MSE: {np.mean(df_metrics['mse'])} - RMSE: {np.mean(df_metrics['rmse'])} - MAPE: {np.mean(df_metrics['mape'])}\")\n",
    "print(f\"CustomMetric: {np.mean(df_metrics['metric'])}\")\n",
    "df_metrics\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV - Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_generator=test_patients = list(df_test['Patient'].unique())\n",
    "dict_predictions = {\n",
    "    'Patient': [],\n",
    "    'target': [],\n",
    "    'prediction': [],\n",
    "    'confidences' : []\n",
    "}\n",
    "\n",
    "for patient in tqdm(test_patients, position=0):\n",
    "    subset = df_train[df_train['Patient']==patient]\n",
    "    list_weeks_elapsed = dict_train_sequence_weekssincelastvisit[patient]\n",
    "    list_weeks_cum = list(dict_train_sequence_cumweeks[patient])\n",
    "    list_fvc_sequence = np.array(dict_train_sequence_fvc[patient])\n",
    "    \n",
    "    list_results = [model.predictEvaluateModel(X_generator=X_val_generator,\n",
    "                                              patient=patient, \n",
    "                                              list_weeks_elapsed=list_weeks_elapsed,\n",
    "                                              list_weeks_since_firstvisit=list_weeks_cum,\n",
    "                                              initial_fvc=[dict_patients_train_ini_features[patient]['FVC']])[0].numpy().flatten()\n",
    "                    for model in list_models]\n",
    "    array_results = np.asarray(list_results)\n",
    "    result = array_results.mean(axis=0)\n",
    "    stds = 100 - array_results.std(axis=0)\n",
    "\n",
    "    predictions, confidences = np.empty(len(list_fvc_sequence)+1), np.empty(len(list_fvc_sequence)+1)\n",
    "    targets = np.empty(len(list_fvc_sequence)+1)\n",
    "    predictions[0], confidences[0] = dict_patients_test_ini_features[patient]['FVC'], 100.0\n",
    "    targets[0] = dict_patients_test_ini_features[patient]['FVC']\n",
    "    predictions[1:] = result\n",
    "    confidences[1:] = stds\n",
    "    targets[1:] = list_fvc_sequence\n",
    "    \n",
    "    dict_predictions['Patient'].append(patient)\n",
    "    dict_predictions['target'].append(unscale(targets, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['prediction'].append(unscale(predictions, mean_fvc, std_fvc).astype(int))\n",
    "    dict_predictions['confidences'].append(confidences)\n",
    "    \n",
    "    df_metrics = pd.DataFrame({'Target': dict_predictions['target'], \n",
    "                               'Pred': dict_predictions['prediction'],\n",
    "                               'Confidences' :dict_predictions['confidences']}, \n",
    "                                index=[dict_predictions['Patient']])\n",
    "    \n",
    "\n",
    "df_metrics['mse'] = df_metrics.apply(lambda x: np.mean((x['Target'] - x['Pred'])**2), axis=1)\n",
    "df_metrics['rmse'] = df_metrics.apply(lambda x: np.sqrt(np.mean(abs(x['Target'] - x['Pred']))) ,axis=1)\n",
    "df_metrics['mape'] = df_metrics.apply(lambda x: 100.0 * np.mean(abs((x['Target'] - x['Pred'])/x['Target'])),axis=1)\n",
    "df_metrics['metric'] = df_metrics.apply(lambda x: np.mean([customLossFunction(x['Target'][i], \n",
    "                                                                              x['Pred'][i],\n",
    "                                                                              std=x['Confidences'][i]).numpy() \n",
    "                                                           for i in range(len(x['Pred']))]),\n",
    "                                  axis=1)\n",
    "\n",
    "print('==='*20)\n",
    "print('Metrics: ')\n",
    "print(f\"MSE: {np.mean(df_metrics['mse'])} - RMSE: {np.mean(df_metrics['rmse'])} - MAPE: {np.mean(df_metrics['mape'])}\")\n",
    "print(f\"CustomMetric: {np.mean(df_metrics['metric'])}\")\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions  \n",
    "\n",
    "- The Sequence to Sequence is able to forecast long sequences and dynamic, one of the biggest challenges on inference stage is the wide range of week a patient can attend the doctor.\n",
    "\n",
    "- What can the model do that a baseline can not? \n",
    "    - Offer good forecasts for **long time fvc measurements** with accurate results, without depending of a fix windowing time visits.\n",
    "\n",
    "    - **Interpret how the CT-Scan is affecting the forecasts.** Many linear/gradient models can exploit linear relationships between patients metadata and the elapsed time between visits with FVC measure, but our model is using CT-Scan features along with metadata to perform reliable and confident results that can answer questions and not only make forecasts. \n",
    "    \n",
    "- Regularitzation and Data Augmentation is strongly important due to the lack of data we have in our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
