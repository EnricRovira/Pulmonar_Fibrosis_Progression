{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EncoderDecoder Sequence Fibrosis Progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# 01. Libraries\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# To allocate memory dynamically\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "# tf.config.experimental.enable_mlir_graph_optimization()\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers, constraints, initializers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from Utils.utils import *\n",
    "from Utils.attention_layers import BahdanauAttention, ScaledDotProductAttention, GeneralAttention, VisualAttentionBlock\n",
    "from Utils.preprocess_scans import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import xgboost as xgb\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# 02. Global Variables\n",
    "\n",
    "path = '../../01_Data/'\n",
    "path_models = '../../05_Saved_Models/'\n",
    "\n",
    "path_train_masks = path + '/train_masks_fast_masks/'\n",
    "path_test_masks = path + '/test_masks_fast_masks/'\n",
    "\n",
    "path_scans_train = path + 'train/'\n",
    "path_scans_test = path + 'test/'\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 176/176 [00:00<00:00, 44126.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 4999.17it/s]\n",
      " 23%|██████████████████▋                                                             | 41/176 [00:00<00:00, 403.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 -> There are 176 train unique patients\n",
      "1.2 -> There are 5 test unique patients\n",
      "No. of Train Masks : 176\n",
      "No. of Test Masks : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 176/176 [00:00<00:00, 463.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 313.31it/s]\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# 03. Load Data & Preprocess Data\n",
    "\n",
    "df_train = pd.read_csv( path + 'train.csv')\n",
    "df_test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "print(f'1.1 -> There are {df_train.Patient.unique().shape[0]} train unique patients')\n",
    "print(f'1.2 -> There are {df_test.Patient.unique().shape[0]} test unique patients')\n",
    "\n",
    "train_mask_paths = glob.glob(path_train_masks + '*')\n",
    "test_mask_paths = glob.glob(path_test_masks + '*')\n",
    "\n",
    "print(f'No. of Train Masks : {len(train_mask_paths)}')\n",
    "print(f'No. of Test Masks : {len(test_mask_paths)}')\n",
    "      \n",
    "unique_train_patients = df_train.Patient.unique()\n",
    "unique_test_patients = df_test.Patient.unique()\n",
    "\n",
    "train_patients = os.listdir(path_train_masks)\n",
    "test_patients = os.listdir(path_test_masks)\n",
    "\n",
    "dict_train_patients_masks_paths = {patient: path_train_masks + patient + '/' for patient in train_patients}\n",
    "dict_test_patients_masks_paths = {patient: path_test_masks + patient + '/' for patient in test_patients}\n",
    "\n",
    "dict_train_patients_scans_paths = {patient: path_scans_train + patient + '/' for patient in unique_train_patients}\n",
    "dict_test_patients_scans_paths = {patient: path_scans_test + patient + '/' for patient in unique_test_patients}\n",
    "\n",
    "for patient in tqdm(dict_train_patients_masks_paths):\n",
    "    list_files = os.listdir(dict_train_patients_masks_paths[patient])\n",
    "    list_files = [dict_train_patients_masks_paths[patient] + file for file in list_files]\n",
    "    dict_train_patients_masks_paths[patient] = list_files\n",
    "    \n",
    "for patient in tqdm(dict_test_patients_masks_paths):\n",
    "    list_files = os.listdir(dict_test_patients_masks_paths[patient])\n",
    "    list_files = [dict_test_patients_masks_paths[patient] + file for file in list_files]\n",
    "    dict_test_patients_masks_paths[patient] = list_files\n",
    "    \n",
    "\n",
    "for patient in tqdm(dict_train_patients_scans_paths):\n",
    "    list_files = os.listdir(dict_train_patients_scans_paths[patient])\n",
    "    list_files = [dict_train_patients_scans_paths[patient] + file for file in list_files]\n",
    "    dict_train_patients_scans_paths[patient] = list_files\n",
    "    \n",
    "for patient in tqdm(dict_test_patients_scans_paths):\n",
    "    list_files = os.listdir(dict_test_patients_scans_paths[patient])\n",
    "    list_files = [dict_test_patients_scans_paths[patient] + file for file in list_files]\n",
    "    dict_test_patients_scans_paths[patient] = list_files\n",
    "    \n",
    "# Preprocessing:\n",
    "\n",
    "df_train = df_train.groupby(['Patient', 'Weeks']).agg({\n",
    "    'FVC': np.mean,\n",
    "    'Percent': np.mean,\n",
    "    'Age': np.max,\n",
    "    'Sex': np.max,\n",
    "    'SmokingStatus': np.max \n",
    "}).reset_index()\n",
    "\n",
    "df_train['FVC_Percent'] = (df_train['FVC'] / df_train['Percent']) * 100\n",
    "df_test['FVC_Percent'] = (df_test['FVC'] / df_test['Percent']) * 100\n",
    "\n",
    "\n",
    "# Standarize data\n",
    "\n",
    "mean_fvc, std_fvc = df_train.FVC.mean(), df_train.FVC.std()\n",
    "mean_perc, std_perc = df_train.Percent.mean(), df_train.Percent.std()\n",
    "mean_age, std_age = df_train.Age.mean(), df_train.Age.std()\n",
    "\n",
    "df_train['Age'] = df_train['Age'].apply(lambda x: (x-mean_age)/std_age)\n",
    "df_test['Age'] = df_test['Age'].apply(lambda x: (x-mean_age)/std_age)\n",
    "\n",
    "df_train['FVC'] = df_train['FVC'].apply(lambda x: (x-mean_fvc)/std_fvc)\n",
    "df_test['FVC'] = df_test['FVC'].apply(lambda x: (x-mean_fvc)/std_fvc)\n",
    "df_train['FVC_Percent'] = df_train['FVC_Percent'].apply(lambda x: (x-mean_fvc)/std_fvc)\n",
    "df_test['FVC_Percent'] = df_test['FVC_Percent'].apply(lambda x: (x-mean_fvc)/std_fvc)\n",
    "\n",
    "df_train['Percent'] = df_train['Percent'].apply(lambda x: (x-mean_perc)/std_perc)\n",
    "df_test['Percent'] = df_test['Percent'].apply(lambda x: (x-mean_perc)/std_perc)\n",
    "\n",
    "# Mapping categories dictionaries \n",
    "\n",
    "dict_sex = {'Male': 0, 'Female': 1}\n",
    "dict_sex_inv = {0: 'Male', 1: 'Female'}\n",
    "\n",
    "dict_smoke = {'Ex-smoker': 0, 'Never smoked': 1, 'Currently smokes': 2}\n",
    "dict_smoke_inv = {0: 'Ex-smoker', 1:'Never smoked', 2:'Currently smokes'}\n",
    "\n",
    "dict_kind_patient = {'decreased': 0, 'regular': 1, 'increased': 2}\n",
    "dict_kind_patient_inv = {0: 'decreased', 1: 'regular', 2: 'increased'}\n",
    "\n",
    "df_train.Sex = df_train.Sex.apply(lambda x: dict_sex[x])\n",
    "df_train.SmokingStatus = df_train.SmokingStatus.apply(lambda x: dict_smoke[x])\n",
    "\n",
    "df_test.Sex = df_test.Sex.apply(lambda x: dict_sex[x])\n",
    "df_test.SmokingStatus = df_test.SmokingStatus.apply(lambda x: dict_smoke[x])\n",
    "\n",
    "# Build WeeksSinceLastVisit feature\n",
    "\n",
    "df_train['ElapsedWeeks'] = df_train['Weeks']\n",
    "df_test['ElapsedWeeks'] = df_test['Weeks']\n",
    "\n",
    "train_weeks_elapsed = df_train.set_index(['Patient', 'Weeks'])['ElapsedWeeks'].diff().reset_index()\n",
    "test_weeks_elapsed = df_test.set_index(['Patient', 'Weeks'])['ElapsedWeeks'].diff().reset_index()\n",
    "\n",
    "df_train = df_train.drop('ElapsedWeeks', axis=1)\n",
    "df_test = df_test.drop('ElapsedWeeks', axis=1)\n",
    "\n",
    "train_weeks_elapsed['ElapsedWeeks'] = train_weeks_elapsed['ElapsedWeeks'].fillna(0).astype(int)\n",
    "test_weeks_elapsed['ElapsedWeeks'] = test_weeks_elapsed['ElapsedWeeks'].fillna(0).astype(int)\n",
    "\n",
    "df_train = df_train.merge(train_weeks_elapsed, how='inner', on=['Patient', 'Weeks'])\n",
    "df_test = df_test.merge(test_weeks_elapsed, how='inner', on=['Patient', 'Weeks'])\n",
    "\n",
    "df_train['patient_row'] = df_train.sort_values(['Patient', 'Weeks'], ascending=[True, True]) \\\n",
    "             .groupby(['Patient']) \\\n",
    "             .cumcount() + 1\n",
    "\n",
    "df_test['patient_row'] = df_test.sort_values(['Patient', 'Weeks'], ascending=[True, True]) \\\n",
    "             .groupby(['Patient']) \\\n",
    "             .cumcount() + 1\n",
    "\n",
    "df_train['WeeksSinceLastVisit'] = df_train.apply(lambda x: x['Weeks'] if x['patient_row']==1 else x['ElapsedWeeks'], axis=1)\n",
    "df_test['WeeksSinceLastVisit'] = df_test.apply(lambda x: x['Weeks'] if x['patient_row']==1 else x['ElapsedWeeks'], axis=1)\n",
    "\n",
    "# Norm Weeks\n",
    "\n",
    "mean_weeks, std_weeks = df_train.Weeks.mean(), df_train.Weeks.std()\n",
    "\n",
    "df_train['WeeksSinceLastVisit'] = df_train['WeeksSinceLastVisit'].apply(lambda x: (x-mean_weeks)/std_weeks)\n",
    "df_test['WeeksSinceLastVisit'] = df_test['WeeksSinceLastVisit'].apply(lambda x: (x-mean_weeks)/std_weeks)\n",
    "\n",
    "\n",
    "df_train['Weeks'] = df_train['Weeks'].apply(lambda x: (x-mean_weeks)/std_weeks)\n",
    "df_test['Weeks'] = df_test['Weeks'].apply(lambda x: (x-mean_weeks)/std_weeks)\n",
    "\n",
    "# Ini dictionaries\n",
    "\n",
    "columns = ['FVC', 'Age', 'Sex', 'SmokingStatus', 'WeeksSinceLastVisit', 'Percent']\n",
    "dict_patients_train_ini_features, dict_patients_test_ini_features = {}, {}\n",
    "dict_patients_train_kind_patient, dict_patients_test_kind_patient = {}, {}\n",
    "df_train_patients, df_test_patients = df_train.set_index('Patient'), df_test.set_index('Patient')\n",
    "\n",
    "for patient in unique_train_patients:\n",
    "    dict_patients_train_ini_features[patient] = df_train_patients[columns][df_train_patients.index==patient].\\\n",
    "                                                                    to_dict('records')[0]\n",
    "    std = np.std(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values)\n",
    "    mean_first_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[:1])\n",
    "    mean_last_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[-1:])\n",
    "    if std<=100:\n",
    "        dict_patients_train_kind_patient[patient] = 'regular'\n",
    "    elif std>100 and mean_last_1 > mean_first_1 :\n",
    "        dict_patients_train_kind_patient[patient] = 'increased'\n",
    "    elif std>100 and mean_last_1 <= mean_first_1 :\n",
    "        dict_patients_train_kind_patient[patient] = 'decreased'\n",
    "    dict_patients_train_ini_features[patient]['kind'] = dict_kind_patient[dict_patients_train_kind_patient[patient]]\n",
    "        \n",
    "    \n",
    "for patient in unique_test_patients:\n",
    "    dict_patients_test_ini_features[patient] = df_test_patients[columns][df_test_patients.index==patient].\\\n",
    "                                                                    to_dict('records')[0]\n",
    "    std = np.std(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values)\n",
    "    mean_first_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[:1])\n",
    "    mean_last_1 = np.mean(unscale(df_train_patients['FVC'][df_train_patients.index==patient], mean_fvc, std_fvc).values[-1:])\n",
    "    if std<=100:\n",
    "        dict_patients_test_kind_patient[patient] = 'regular'\n",
    "    elif std>100 and mean_last_1 > mean_first_1 :\n",
    "        dict_patients_test_kind_patient[patient] = 'increased'\n",
    "    elif std>100 and mean_last_1 <= mean_first_1 :\n",
    "        dict_patients_test_kind_patient[patient] = 'decreased'\n",
    "    dict_patients_test_ini_features[patient]['kind'] = dict_kind_patient[dict_patients_test_kind_patient[patient]]\n",
    "\n",
    "# Decoder inputs\n",
    "\n",
    "dict_train_sequence_fvc, dict_train_sequence_weekssincelastvisit = {}, {}\n",
    "dict_train_sequence_cumweeks = {}\n",
    "for patient in unique_train_patients:\n",
    "    dict_train_sequence_fvc[patient] = list(df_train_patients['FVC'].loc[patient].values[1:])\n",
    "    dict_train_sequence_weekssincelastvisit[patient] = list(df_train_patients['WeeksSinceLastVisit'].loc[patient].values[1:])\n",
    "    dict_train_sequence_cumweeks[patient] = list(df_train_patients['Weeks'].loc[patient].values[1:])\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(list_patients, dict_ini_features, dict_seq_weeks, dict_seq_cumweeks, \n",
    "                 training=True, predictions=None):\n",
    "    \n",
    "    dict_to_tree = {\n",
    "        'Patient' : [],\n",
    "        'Weeks_Elapsed_since_firstVisit': [],\n",
    "        'Base_Percent' : [],\n",
    "        'Age' : [],\n",
    "        'Sex' : [],\n",
    "        'Base_Week' : [],\n",
    "        'Base_FVC' : [],\n",
    "        'Curr_Smokes' : [],\n",
    "        'Ex_Smoker' : [],\n",
    "        'Never_Smoked' : []\n",
    "    }\n",
    "\n",
    "    if training:\n",
    "        dict_to_tree['fvc_real'] = []\n",
    "        dict_to_tree['kind'] = []\n",
    "    \n",
    "\n",
    "    for patient in tqdm(list_patients, position=0):\n",
    "        \n",
    "        dict_to_tree['Weeks_Elapsed_since_firstVisit'].extend([dict_seq_cumweeks[patient][i] \\\n",
    "                                            for i in range(len(dict_seq_cumweeks[patient]))])\n",
    "        \n",
    "        for i in range(len(dict_seq_weeks[patient])):\n",
    "            dict_to_tree['Patient'].extend([patient])\n",
    "\n",
    "            dict_to_tree['Base_Percent'].extend([dict_ini_features[patient]['Percent']])\n",
    "\n",
    "            dict_to_tree['Age'].extend([dict_ini_features[patient]['Age']])\n",
    "\n",
    "            dict_to_tree['Sex'].extend([dict_ini_features[patient]['Sex']])\n",
    "\n",
    "            dict_to_tree['Base_Week'].extend([dict_ini_features[patient]['WeeksSinceLastVisit']])\n",
    "\n",
    "            dict_to_tree['Base_FVC'].extend([dict_ini_features[patient]['FVC']])\n",
    "\n",
    "            dict_to_tree['Curr_Smokes'].extend([1 if dict_ini_features[patient]['SmokingStatus']==2 else 0])\n",
    "\n",
    "            dict_to_tree['Ex_Smoker'].extend([1 if dict_ini_features[patient]['SmokingStatus']==0 else 0])\n",
    "\n",
    "            dict_to_tree['Never_Smoked'].extend([1 if dict_ini_features[patient]['SmokingStatus']==1 else 0])\n",
    "            \n",
    "            if training:\n",
    "                dict_to_tree['kind'].extend([dict_ini_features[patient]['kind']])\n",
    "\n",
    "        list_weeks_elapsed = list(dict_seq_weeks[patient])\n",
    "        list_weeks_cum = list(dict_seq_cumweeks[patient])\n",
    "\n",
    "        if training:\n",
    "            dict_to_tree['fvc_real'].extend(dict_train_sequence_fvc[patient])\n",
    "\n",
    "    df_tree = pd.DataFrame.from_dict(dict_to_tree, orient='columns')\n",
    "    \n",
    "    return df_tree\n",
    "\n",
    "\n",
    "def buildTrainModel(dict_params, features, df_train, df_val, epochs, verbose_eval=10):\n",
    "    X_train, y_train = df_train[features], df_train['Confidence']\n",
    "    X_val, y_val = df_val[features], df_val['Confidence']\n",
    "    \n",
    "    xgb_data = [(xgb.DMatrix(X_train, y_train), 'train'), (xgb.DMatrix(X_val, y_val), 'valid')]\n",
    "   \n",
    "    xgb_model = xgb.train(\n",
    "                        params=dict_params,\n",
    "                        dtrain=xgb.DMatrix(X_train, y_train),\n",
    "                        num_boost_round=epochs,\n",
    "                        evals=xgb_data,\n",
    "                        verbose_eval=verbose_eval,\n",
    "                        early_stopping_rounds=100\n",
    "                        \n",
    "    )\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "\n",
    "def lossFuncWeights(weight, row):\n",
    "    confidence = weight\n",
    "    sigma_clipped = max(confidence, 70)\n",
    "    diff = np.abs(row['fvc_real'] - row['fvc_pred'])\n",
    "    delta = min(diff, 1000)\n",
    "    score = -np.sqrt(2)*delta/sigma_clipped - np.log(np.sqrt(2)*sigma_clipped)\n",
    "    return -score\n",
    "\n",
    "\n",
    "def getConfidenceWeights(df):\n",
    "    results = []\n",
    "    tk0 = tqdm(df.iterrows(), total=len(df), position=0)\n",
    "    for _, row in tk0:\n",
    "        loss_partial = partial(lossFuncWeights, row=row)\n",
    "        weight = [100]\n",
    "        result = sp.optimize.minimize(loss_partial, weight, method='SLSQP')\n",
    "        x = result['x']\n",
    "        results.append(x[0])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = unscale(y_true, mean_fvc, std_fvc)\n",
    "        y_pred = unscale(y_pred, mean_fvc, std_fvc)\n",
    "        return _lambda * quantileLoss(tf.constant([0.2, 0.5, 0.8]), y_true, y_pred) + (1 - _lambda)*customLossFunction(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def buildModel(num_inputs, lambda_factor):\n",
    "    z = layers.Input((num_inputs,), name=\"Patient\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"d1\")(z)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\",  name=\"d2\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    p1 = layers.Dense(3, activation=\"linear\", name=\"p1\")(x)\n",
    "    p2 = layers.Dense(3, activation=\"relu\", name=\"p2\")(x)\n",
    "    preds = layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
    "                     name=\"preds\")([p1, p2])\n",
    "\n",
    "    model = models.Model(z, p1, name=\"CNN\")\n",
    "    model_loss = mloss(lambda_factor)\n",
    "    model.compile(loss=model_loss, \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                                     amsgrad=False, clipvalue=10), \n",
    "                  metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_inputs = {\n",
    "    'objective': 'reg:squarederror', \n",
    "    'eta': 0.01, \n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.9, \n",
    "    'gamma': 0.4, \n",
    "    'booster' : 'gblinear',\n",
    "    'eval_metric': 'rmse', \n",
    "    'seed': 12 \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model FVC Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 75157.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 26076.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Fold: 1\n",
      "Train patients: 150, Test patients: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|███▌                                                                           | 61/1366 [00:00<00:02, 605.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 6.937567710876465\n",
      "Metric train+val, before confidence weights: 6.8172926902771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 609.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.385497570037842\n",
      "[22:03:49] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:290.34781\tvalid-rmse:341.98206\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:204.15157\tvalid-rmse:252.90662\n",
      "[100]\ttrain-rmse:196.63069\tvalid-rmse:241.17549\n",
      "[150]\ttrain-rmse:195.32513\tvalid-rmse:238.97166\n",
      "[200]\ttrain-rmse:194.86775\tvalid-rmse:238.30144\n",
      "[250]\ttrain-rmse:194.65857\tvalid-rmse:237.94781\n",
      "[300]\ttrain-rmse:194.54894\tvalid-rmse:237.69423\n",
      "[350]\ttrain-rmse:194.48521\tvalid-rmse:237.49904\n",
      "[400]\ttrain-rmse:194.44498\tvalid-rmse:237.35043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\ttrain-rmse:194.41777\tvalid-rmse:237.23991\n",
      "[500]\ttrain-rmse:194.39835\tvalid-rmse:237.16054\n",
      "[550]\ttrain-rmse:194.38382\tvalid-rmse:237.10548\n",
      "[600]\ttrain-rmse:194.37254\tvalid-rmse:237.06906\n",
      "[650]\ttrain-rmse:194.36354\tvalid-rmse:237.04675\n",
      "[700]\ttrain-rmse:194.35617\tvalid-rmse:237.03493\n",
      "[750]\ttrain-rmse:194.35007\tvalid-rmse:237.03075\n",
      "[799]\ttrain-rmse:194.34505\tvalid-rmse:237.03201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Validation Weights predicted: 6.964028358459473\n",
      "************************************************************\n",
      "Num Fold: 2\n",
      "Train patients: 151, Test patients: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 50469.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 25061.57it/s]\n",
      "  4%|███▍                                                                           | 59/1366 [00:00<00:02, 574.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 7.060276031494141\n",
      "Metric train+val, before confidence weights: 6.80912971496582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 620.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.3090691566467285\n",
      "[22:03:55] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:271.61566\tvalid-rmse:380.50568\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:197.17854\tvalid-rmse:283.34824\n",
      "[100]\ttrain-rmse:190.29198\tvalid-rmse:265.08493\n",
      "[150]\ttrain-rmse:188.98406\tvalid-rmse:260.41141\n",
      "[200]\ttrain-rmse:188.50290\tvalid-rmse:258.98560\n",
      "[250]\ttrain-rmse:188.27588\tvalid-rmse:258.52536\n",
      "[300]\ttrain-rmse:188.15433\tvalid-rmse:258.39533\n",
      "[350]\ttrain-rmse:188.08266\tvalid-rmse:258.38678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttrain-rmse:188.03671\tvalid-rmse:258.42184\n",
      "Stopping. Best iteration:\n",
      "[328]\ttrain-rmse:188.11002\tvalid-rmse:258.38214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 50465.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Validation Weights predicted: 7.109722137451172\n",
      "************************************************************\n",
      "Num Fold: 3\n",
      "Train patients: 151, Test patients: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                           | 57/1366 [00:00<00:02, 565.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 7.181115627288818\n",
      "Metric train+val, before confidence weights: 6.807025909423828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 607.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.337792873382568\n",
      "[22:04:02] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:265.45761\tvalid-rmse:435.42719\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:185.40353\tvalid-rmse:332.31299\n",
      "[100]\ttrain-rmse:176.98212\tvalid-rmse:311.84192\n",
      "[150]\ttrain-rmse:175.24812\tvalid-rmse:306.85336\n",
      "[200]\ttrain-rmse:174.63496\tvalid-rmse:305.36398\n",
      "[250]\ttrain-rmse:174.37321\tvalid-rmse:304.80383\n",
      "[300]\ttrain-rmse:174.25064\tvalid-rmse:304.53674\n",
      "[350]\ttrain-rmse:174.18919\tvalid-rmse:304.38290\n",
      "[400]\ttrain-rmse:174.15665\tvalid-rmse:304.28265\n",
      "[450]\ttrain-rmse:174.13863\tvalid-rmse:304.21195\n",
      "[500]\ttrain-rmse:174.12826\tvalid-rmse:304.15936\n",
      "[550]\ttrain-rmse:174.12204\tvalid-rmse:304.11896\n",
      "[600]\ttrain-rmse:174.11813\tvalid-rmse:304.08673\n",
      "[650]\ttrain-rmse:174.11554\tvalid-rmse:304.06055\n",
      "[700]\ttrain-rmse:174.11371\tvalid-rmse:304.03879\n",
      "[750]\ttrain-rmse:174.11238\tvalid-rmse:304.02035\n",
      "[799]\ttrain-rmse:174.11134\tvalid-rmse:304.00479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 50469.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Validation Weights predicted: 7.436273097991943\n",
      "************************************************************\n",
      "Num Fold: 4\n",
      "Train patients: 151, Test patients: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▌                                                                       | 115/1366 [00:00<00:02, 564.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 6.9726762771606445\n",
      "Metric train+val, before confidence weights: 6.820812225341797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 611.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.3480024337768555\n",
      "[22:04:08] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:282.74722\tvalid-rmse:343.82190\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:200.60596\tvalid-rmse:249.20476\n",
      "[100]\ttrain-rmse:192.71001\tvalid-rmse:238.89645\n",
      "[150]\ttrain-rmse:191.30045\tvalid-rmse:238.32460\n",
      "[200]\ttrain-rmse:190.81342\tvalid-rmse:238.90187\n",
      "Stopping. Best iteration:\n",
      "[133]\ttrain-rmse:191.60345\tvalid-rmse:238.24963\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 50465.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 25067.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Validation Weights predicted: 6.989419937133789\n",
      "************************************************************\n",
      "Num Fold: 5\n",
      "Train patients: 151, Test patients: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                           | 59/1366 [00:00<00:02, 585.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 7.0418620109558105\n",
      "Metric train+val, before confidence weights: 6.850786209106445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 603.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.384232521057129\n",
      "[22:04:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:291.16080\tvalid-rmse:362.56149\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:208.63739\tvalid-rmse:277.96130\n",
      "[100]\ttrain-rmse:201.92964\tvalid-rmse:261.04028\n",
      "[150]\ttrain-rmse:200.78026\tvalid-rmse:254.04828\n",
      "[200]\ttrain-rmse:200.36670\tvalid-rmse:250.24649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttrain-rmse:200.18076\tvalid-rmse:247.98436\n",
      "[300]\ttrain-rmse:200.08897\tvalid-rmse:246.57173\n",
      "[350]\ttrain-rmse:200.03981\tvalid-rmse:245.65364\n",
      "[400]\ttrain-rmse:200.01114\tvalid-rmse:245.03163\n",
      "[450]\ttrain-rmse:199.99298\tvalid-rmse:244.59050\n",
      "[500]\ttrain-rmse:199.98050\tvalid-rmse:244.26186\n",
      "[550]\ttrain-rmse:199.97131\tvalid-rmse:244.00485\n",
      "[600]\ttrain-rmse:199.96416\tvalid-rmse:243.79483\n",
      "[650]\ttrain-rmse:199.95833\tvalid-rmse:243.61678\n",
      "[700]\ttrain-rmse:199.95343\tvalid-rmse:243.46140\n",
      "[750]\ttrain-rmse:199.94926\tvalid-rmse:243.32295\n",
      "[799]\ttrain-rmse:199.94566\tvalid-rmse:243.20018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 50469.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 25067.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Validation Weights predicted: 7.066259860992432\n",
      "************************************************************\n",
      "Num Fold: 6\n",
      "Train patients: 151, Test patients: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                           | 55/1366 [00:00<00:02, 545.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 6.892035961151123\n",
      "Metric train+val, before confidence weights: 6.821486949920654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 609.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.346269130706787\n",
      "[22:04:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:290.25894\tvalid-rmse:328.71741\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:210.81987\tvalid-rmse:234.73257\n",
      "[100]\ttrain-rmse:203.91798\tvalid-rmse:224.19579\n",
      "[150]\ttrain-rmse:202.73361\tvalid-rmse:222.96739\n",
      "[200]\ttrain-rmse:202.30640\tvalid-rmse:223.15006\n",
      "[250]\ttrain-rmse:202.10687\tvalid-rmse:223.53917\n",
      "Stopping. Best iteration:\n",
      "[158]\ttrain-rmse:202.63864\tvalid-rmse:222.95666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 151/151 [00:00<00:00, 50473.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 25079.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Validation Weights predicted: 6.837316036224365\n",
      "************************************************************\n",
      "Num Fold: 7\n",
      "Train patients: 151, Test patients: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████                                                                       | 124/1366 [00:00<00:02, 611.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric base model: 7.000766754150391\n",
      "Metric train+val, before confidence weights: 6.786550521850586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 615.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric train+val, confidence weights: 6.290842056274414\n",
      "[22:04:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:274.45371\tvalid-rmse:355.73852\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:198.63367\tvalid-rmse:263.89221\n",
      "[100]\ttrain-rmse:191.97582\tvalid-rmse:249.13896\n",
      "[150]\ttrain-rmse:190.73009\tvalid-rmse:245.83853\n",
      "[200]\ttrain-rmse:190.24829\tvalid-rmse:244.90456\n",
      "[250]\ttrain-rmse:190.00981\tvalid-rmse:244.59798\n",
      "[300]\ttrain-rmse:189.87543\tvalid-rmse:244.49878\n",
      "[350]\ttrain-rmse:189.79048\tvalid-rmse:244.48602\n",
      "[400]\ttrain-rmse:189.73076\tvalid-rmse:244.51649\n",
      "Stopping. Best iteration:\n",
      "[334]\ttrain-rmse:189.81413\tvalid-rmse:244.48401\n",
      "\n",
      "************************************************************\n",
      "Validation Weights predicted: 7.106487274169922\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 7, random_state = 12, shuffle = True)\n",
    "list_models, list_history, list_final_metric = [], [], []\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(skf.split(unique_train_patients, \n",
    "                                                              np.zeros(unique_train_patients.shape[0]))):\n",
    "\n",
    "    x_train_patients = list(unique_train_patients[train_index])\n",
    "    x_val_patients = list(unique_train_patients[val_index])\n",
    "    \n",
    "    print(f'Num Fold: {num_fold + 1}')\n",
    "    print(f'Train patients: {len(x_train_patients)}, Test patients: {len(x_val_patients)}')  \n",
    "\n",
    "    df_train_weights = buildDataSet(x_train_patients, \n",
    "                 dict_ini_features=dict_patients_train_ini_features, \n",
    "                 dict_seq_weeks=dict_train_sequence_weekssincelastvisit, \n",
    "                 dict_seq_cumweeks=dict_train_sequence_cumweeks, \n",
    "                 training=True, \n",
    "                 predictions=None)\n",
    "\n",
    "    df_val_weights = buildDataSet(x_val_patients,\n",
    "                 dict_ini_features=dict_patients_train_ini_features, \n",
    "                 dict_seq_weeks=dict_train_sequence_weekssincelastvisit, \n",
    "                 dict_seq_cumweeks=dict_train_sequence_cumweeks, \n",
    "                 training=True, \n",
    "                 predictions=None)\n",
    "                                               \n",
    "    features = list(col for col in df_train_weights.columns if col not in ['Patient', 'fvc_real', 'kind'])   \n",
    "    y_train = df_train_weights['fvc_real'].astype(float)\n",
    "    y_val = df_val_weights['fvc_real'].astype(float)\n",
    "\n",
    "    X_train =  df_train_weights[features]\n",
    "    X_val =  df_val_weights[features]\n",
    "\n",
    "    model_weights = buildModel(len(features), lambda_factor=0.8)\n",
    "\n",
    "    model_weights.fit(X_train, y_train, shuffle=True, batch_size=16, epochs=40, \n",
    "                validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    list_models.append(model_weights)\n",
    "\n",
    "    y_val_pred = model_weights.predict(X_val)\n",
    "    y_val_pred_median = unscale(y_val_pred[:, 1], mean_fvc, std_fvc)\n",
    "    y_val_pred_std = unscale(y_val_pred[:, 2], mean_fvc, std_fvc) - unscale(y_val_pred[:, 0], mean_fvc, std_fvc)\n",
    "\n",
    "    \n",
    "    metric = customLossFunction(unscale(y_val, mean_fvc, std_fvc),\n",
    "                                      y_val_pred_median,\n",
    "                                     y_val_pred_std).numpy()\n",
    "    \n",
    "    list_history.append({'metric' : metric})\n",
    "    print(f'Metric base model: {metric}')\n",
    "    \n",
    "    ### Confidence ###\n",
    "    \n",
    "    df_all_weights = pd.concat([df_train_weights, df_val_weights], axis=0)\n",
    "    df_all_weights = df_all_weights[features + ['fvc_real', 'Patient']]\n",
    "    \n",
    "    predictions = model_weights.predict(df_all_weights[features])\n",
    "    df_all_weights['fvc_real'] = unscale(df_all_weights['fvc_real'], mean_fvc, std_fvc)\n",
    "    df_all_weights['fvc_pred'] = unscale(predictions[:, 1], mean_fvc, std_fvc)\n",
    "    df_all_weights['Confidence'] = unscale(predictions[:, 2], mean_fvc, std_fvc) - unscale(predictions[:, 0], mean_fvc, std_fvc)\n",
    "    df_all_weights['sigma_clipped'] = df_all_weights['Confidence'].apply(lambda x: max(x, 70))\n",
    "    df_all_weights['diff'] = np.abs(df_all_weights['fvc_real'] - df_all_weights['fvc_pred'])\n",
    "    df_all_weights['delta'] = df_all_weights['diff'].apply(lambda x: min(x, 1_000))\n",
    "    df_all_weights['score'] = -np.sqrt(2)*df_all_weights['delta']/df_all_weights['sigma_clipped'] - np.log(np.sqrt(2)*df_all_weights['sigma_clipped'])\n",
    "    \n",
    "    score = customLossFunction(df_all_weights['fvc_real'],\n",
    "                                 df_all_weights['fvc_pred'],\n",
    "                                 df_all_weights['Confidence']).numpy()\n",
    "    print(f'Metric train+val, before confidence weights: {score}') \n",
    "    \n",
    "    confidence_weights = getConfidenceWeights(df_all_weights)\n",
    "    \n",
    "    df_all_weights['Confidence'] = confidence_weights\n",
    "    df_all_weights['sigma_clipped'] = df_all_weights['Confidence'].apply(lambda x: max(x, 70))\n",
    "    df_all_weights['diff'] = np.abs(df_all_weights['fvc_real'] - df_all_weights['fvc_pred'])\n",
    "    df_all_weights['delta'] = df_all_weights['diff'].apply(lambda x: min(x, 1_000))\n",
    "    df_all_weights['score'] = -np.sqrt(2)*df_all_weights['delta']/df_all_weights['sigma_clipped'] - np.log(np.sqrt(2)*df_all_weights['sigma_clipped'])\n",
    "    score = customLossFunction(df_all_weights['fvc_real'],\n",
    "                                 df_all_weights['fvc_pred'],\n",
    "                                 df_all_weights['Confidence']).numpy()\n",
    "    print(f'Metric train+val, confidence weights: {score}') \n",
    "    \n",
    "    # xgboost\n",
    "    \n",
    "    df_tmp_train = df_all_weights[df_all_weights['Patient'].isin(x_train_patients)]\n",
    "    df_tmp_val = df_all_weights[df_all_weights['Patient'].isin(x_val_patients)]\n",
    "    \n",
    "    xgb_model = buildTrainModel(xgb_inputs, features, \\\n",
    "                                df_train=df_tmp_train, df_val=df_tmp_val, epochs=800, verbose_eval=50)\n",
    "    \n",
    "    pred_confidence = xgb_model.predict(xgb.DMatrix(df_tmp_val[features]))\n",
    "    final_metric = customLossFunction(y_true=df_tmp_val['fvc_real'],\n",
    "                             y_pred=df_tmp_val['fvc_pred'],\n",
    "                             std=pred_confidence)\n",
    "    \n",
    "    print('***'*20)\n",
    "    print(f'Validation Weights predicted: {final_metric}')\n",
    "    print('***'*20)\n",
    "    list_final_metric.append(final_metric)\n",
    "    \n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0123286 7.072787\n"
     ]
    }
   ],
   "source": [
    "val_metric = np.mean([history['metric'] for history in list_history])\n",
    "\n",
    "print(val_metric, np.mean(list_final_metric))\n",
    "#7.019793 7.0903406\n",
    "# 7.00121 7.063415\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Confidence Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 176/176 [00:00<00:00, 58829.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.0854690320609555\n"
     ]
    }
   ],
   "source": [
    "df_train_confidence = buildDataSet(unique_train_patients,\n",
    "                                 dict_ini_features=dict_patients_train_ini_features, \n",
    "                                 dict_seq_weeks=dict_train_sequence_weekssincelastvisit, \n",
    "                                 dict_seq_cumweeks=dict_train_sequence_cumweeks, \n",
    "                                 training=True, \n",
    "                                 predictions=None)\n",
    "\n",
    "predictions = np.mean([model.predict(df_train_confidence[features]) for model in list_models], axis=0)\n",
    "df_train_confidence['fvc_real'] = unscale(df_train_confidence['fvc_real'], mean_fvc, std_fvc)\n",
    "df_train_confidence['fvc_pred'] = unscale(predictions[:, 0], mean_fvc, std_fvc)\n",
    "df_train_confidence['Confidence'] = unscale(predictions[:, 2], mean_fvc, std_fvc) - unscale(predictions[:, 0], mean_fvc, std_fvc)\n",
    "df_train_confidence['sigma_clipped'] = df_train_confidence['Confidence'].apply(lambda x: max(x, 70))\n",
    "df_train_confidence['diff'] = np.abs(df_train_confidence['fvc_real'] - df_train_confidence['fvc_pred'])\n",
    "df_train_confidence['delta'] = df_train_confidence['diff'].apply(lambda x: min(x, 1_000))\n",
    "df_train_confidence['score'] = -np.sqrt(2)*df_train_confidence['delta']/df_train_confidence['sigma_clipped'] - np.log(np.sqrt(2)*df_train_confidence['sigma_clipped'])\n",
    "score = df_train_confidence['score'].mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1366/1366 [00:02<00:00, 533.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def loss_func(weight, row):\n",
    "    confidence = weight\n",
    "    sigma_clipped = max(confidence, 70)\n",
    "    diff = abs(row['fvc_real'] - row['fvc_pred'])\n",
    "    delta = min(diff, 1000)\n",
    "    score = -np.sqrt(2)*delta/sigma_clipped - np.log(np.sqrt(2)*sigma_clipped)\n",
    "    return -score\n",
    "\n",
    "results = []\n",
    "tk0 = tqdm(df_train_confidence.iterrows(), total=len(df_train_confidence), position=0)\n",
    "for _, row in tk0:\n",
    "    loss_partial = partial(loss_func, row=row)\n",
    "    weight = [100]\n",
    "    result = sp.optimize.minimize(loss_partial, weight, method='SLSQP')\n",
    "    x = result['x']\n",
    "    results.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.807227016133499\n"
     ]
    }
   ],
   "source": [
    "df_train_confidence['Confidence'] = results\n",
    "df_train_confidence['sigma_clipped'] = df_train_confidence['Confidence'].apply(lambda x: max(x, 70))\n",
    "df_train_confidence['diff'] = np.abs(df_train_confidence['fvc_real'] - df_train_confidence['fvc_pred'])\n",
    "df_train_confidence['delta'] = df_train_confidence['diff'].apply(lambda x: min(x, 1_000))\n",
    "df_train_confidence['score'] = -np.sqrt(2)*df_train_confidence['delta']/df_train_confidence['sigma_clipped'] - np.log(np.sqrt(2)*df_train_confidence['sigma_clipped'])\n",
    "score = df_train_confidence['score'].mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Confidence Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buildTrainModel(dict_params, features, df_train, epochs, verbose_eval=5):\n",
    "#     X_train, y_train = df_train[features], df_train['Confidence']\n",
    "   \n",
    "#     xgb_model = xgb.train(\n",
    "#                         params=dict_params,\n",
    "#                         dtrain=xgb.DMatrix(X_train, y_train),\n",
    "#                         num_boost_round=epochs,\n",
    "#                         verbose_eval=verbose_eval,\n",
    "#                         early_stopping_rounds=50\n",
    "                        \n",
    "#     )\n",
    "\n",
    "#     return xgb_model \n",
    "\n",
    "\n",
    "# xgb_inputs = {\n",
    "#     'objective': 'reg:squarederror', \n",
    "#     'eta': 0.01, \n",
    "#     'max_depth': 8,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.9, \n",
    "#     'alpha': 1.0, \n",
    "#     'min_child_weight' : 2,\n",
    "#     'eval_metric': 'rmse', \n",
    "#     'seed': 12 \n",
    "# }\n",
    "\n",
    "\n",
    "# xgb_model = buildTrainModel(xgb_inputs, features, df_train=df_train_confidence, epochs=400)\n",
    "# pred_confidence = xgb_model.predict(xgb.DMatrix(df_train_confidence[features]))\n",
    "\n",
    "# print(customLossFunction(y_true=df_train_confidence['fvc_real'],\n",
    "#                          y_pred=df_train_confidence['fvc_pred'],\n",
    "#                          std=pred_confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### History models\n",
    "# 1. val_loss - 0.14948401 & val_metric = 7.581691 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0\n",
    "# 2. 1.7004111 7.6539536 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=True & dim=128\n",
    "# 3. 1.9819709 7.499232 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.75 & resnet=False & dim=256 & visuallatt\n",
    "# 4. 1.6195476 7.4542327 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=custom & dim=128\n",
    "# 5. (7.30) 1.5714737 7.306921 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=custom & dim=128 & lrdecay=0.9\n",
    "# 6. (Best - 7.00) | 1.5109245 7.061371 quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & resnet=custom & dim=128 & inidecay=0.5 & lrdecay=0.9\n",
    "# 7. 1.4737307 6.9873514 7.0969524 | quantiles=[0.2, 0.5, 0.8], eps=0, eps_decay=0. lfactor=0.8 & beta_factor=0.6 & resnet=custom & dim=128 & inidecay=0.9 & lrdecay=0.9\n",
    "# 8. 1.4489578 6.906362 6.972941 | Add kind patient feature\n",
    "# 9. 1.425578 6.822274 6.857718 | Add kind and remove dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
